{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobashwall/Tweets_Classification_Research/blob/main/Suicide_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "bgvOflpaGqZs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rzSRAYluBY8"
      },
      "source": [
        "##Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5MPjGzcsWo7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import h5py\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tabulate import tabulate\n",
        "from google.colab import files\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "import psutil\n",
        "import fcntl\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from multiprocessing import Process\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo86_XpAt8gj",
        "outputId": "365ec801-7c0d-4b1e-e59e-512b98096fbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "/usr/bin/xdg-open: 869: www-browser: not found\n",
            "/usr/bin/xdg-open: 869: links2: not found\n",
            "/usr/bin/xdg-open: 869: elinks: not found\n",
            "/usr/bin/xdg-open: 869: links: not found\n",
            "/usr/bin/xdg-open: 869: lynx: not found\n",
            "/usr/bin/xdg-open: 869: w3m: not found\n",
            "xdg-open: no method available for opening 'https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=QpbqbusxUUIufqgUrT-RaxcteUfxJJzY%2FHfjeCb3B8g'\n",
            "/bin/sh: 1: firefox: not found\n",
            "/bin/sh: 1: google-chrome: not found\n",
            "/bin/sh: 1: chromium-browser: not found\n",
            "/bin/sh: 1: open: not found\n",
            "Cannot retrieve auth tokens.\n",
            "Failure(\"Error opening URL:https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=QpbqbusxUUIufqgUrT-RaxcteUfxJJzY%2FHfjeCb3B8g\")\n"
          ]
        }
      ],
      "source": [
        "!sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!sudo apt-get update -qq 2>&1 > /dev/null\n",
        "!sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n",
        "!google-drive-ocamlfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duWBLwblt_-c",
        "outputId": "1c6541e6-1bf3-4298-9b2d-eeba87815af6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package w3m.\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 128290 files and directories currently installed.)\n",
            "Preparing to unpack .../w3m_0.5.3-37ubuntu0.1_amd64.deb ...\n",
            "Unpacking w3m (0.5.3-37ubuntu0.1) ...\n",
            "Setting up w3m (0.5.3-37ubuntu0.1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n",
            "/content\n",
            "/content/drive\n",
            "/content\n",
            "/\n",
            "Access token retrieved correctly.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -qq w3m # to act as web browser \n",
        "!xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng2RiCQxwjqi"
      },
      "source": [
        "##Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEmoZ5TPwDpW"
      },
      "source": [
        "###Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8X62ZKu0kfW"
      },
      "outputs": [],
      "source": [
        "PERCENT = 0.3\n",
        "DATASETS= [\"first\" #,\"second\", \"third\"\n",
        "           ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoygQTpuxvDr"
      },
      "outputs": [],
      "source": [
        "def convert_class_to_binary(text):\n",
        "  if text == \"suicide\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def write_vectors(dataset):\n",
        "  print(dataset)\n",
        "  if dataset == \"first\":\n",
        "    print(len(first.index))\n",
        "    x_train, x_test, y_train, y_test = train_test_split(first[\"text\"], first[\"is_suicide\"], test_size=PERCENT, random_state=42)\n",
        "  if dataset == \"second\":\n",
        "    print(len(second.index))\n",
        "    x_train, x_test, y_train, y_test = train_test_split(second[\"text\"], second[\"is_suicide\"], test_size=PERCENT, random_state=42)\n",
        "  if dataset == \"third\":\n",
        "    print(len(third.index))\n",
        "    x_train, x_test, y_train, y_test = train_test_split(third[\"text\"], third[\"is_suicide\"], test_size=PERCENT, random_state=42)\n",
        "\n",
        "  print(f'train: {len(y_train)}')\n",
        "  print(f'test: {len(y_test)}')\n",
        "\n",
        "  x_train = pd.DataFrame(x_train)\n",
        "  x_test = pd.DataFrame(x_test)\n",
        "  y_train = pd.DataFrame(y_train)\n",
        "  y_test = pd.DataFrame(y_test)\n",
        "  x_train.to_hdf(f'/content/drive/MyDrive/suicide_model_files/data/{dataset}/train_text.h5', key='dataset', mode='w')\n",
        "  x_test.to_hdf(f'/content/drive/MyDrive/suicide_model_files/data/{dataset}/test_text.h5', key='dataset', mode='w')\n",
        "  y_train.to_hdf(f'/content/drive/MyDrive/suicide_model_files/data/{dataset}/train_class.h5', key='dataset', mode='w')\n",
        "  y_test.to_hdf(f'/content/drive/MyDrive/suicide_model_files/data/{dataset}/test_class.h5', key='dataset', mode='w')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H1NommkwMVz",
        "outputId": "b920572b-440f-40e2-f0af-f4836dcbfd60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first\n",
            "20000\n",
            "train: 14000\n",
            "test: 6000\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "first = pd.read_csv('/content/drive/MyDrive/suicide_model_files/data/1.csv', usecols= [\"text\",\"class\"] ,encoding='latin-1')\n",
        "first[\"class\"] = first[\"class\"].apply(convert_class_to_binary)\n",
        "first= first.rename(columns={\"class\":\"is_suicide\"})\n",
        "first = first.dropna()\n",
        "'''\n",
        "first = pd.read_csv('/content/drive/MyDrive/suicide_model_files/data/1_mini.csv', usecols= [\"text\",\"label\"] ,encoding='latin-1')\n",
        "first= first.rename(columns={\"label\":\"is_suicide\"})\n",
        "first = first.dropna()\n",
        "\n",
        "second = pd.read_csv('/content/drive/MyDrive/suicide_model_files/data/2.csv', usecols= [\"selftext\",\"is_suicide\"],encoding='latin-1')\n",
        "second= second.rename(columns={\"selftext\":\"text\"})\n",
        "second = second.dropna()\n",
        "\n",
        "third =  pd.read_csv('/content/drive/MyDrive/suicide_model_files/data/3.csv', usecols= [\"tweet\",\"intention\"],encoding='latin-1')\n",
        "third = third.rename(columns={\"tweet\":\"text\"})\n",
        "third = third.rename(columns={\"intention\":\"is_suicide\"})\n",
        "third = third.dropna()\n",
        "\n",
        "for ds in DATASETS:\n",
        "  write_vectors(ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS4UouKC2Q5e"
      },
      "source": [
        "###TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMnLVkOi2YV7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "915616bc-aef3-4a86-b6c4-8b92d42596fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nSPECIAL_CHARCATERS_REMOVAL=r\"(?u)\\x08\\\\w+[\"\\'״]*\\\\w\\x08\" # tokenize by \" \" and \"/n\" and allow abbreviations\\n\\n# Gets all the stop words from the file\\npattern = re.compile(SPECIAL_CHARCATERS_REMOVAL)\\nwith open(\"/content/drive/MyDrive/model_files/data/stop_words.txt\", \\'r\\') as f:\\n        STOP_WORDS = pattern.findall(f.read())\\n\\n# Insert all the abbreviations into a dictionary\\nABBREVIATIONS = {}\\nwith open(\"/content/drive/MyDrive/model_files/data/abbreviations.txt\") as f:\\n    for line in f:\\n        key, value = line.split(\"-\")\\n        ABBREVIATIONS[key.strip()] = value.strip()\\n\\n# To preprocess open abbrevaitons\\ndef expand_abbreviations(text):\\n    \"\"\"\\n    a function that gets a text and replace all abbreviations with thier expnaded form\\n    \"\"\"\\n    for match in re.finditer(SPECIAL_CHARCATERS_REMOVAL, text):\\n        abbreviation = match.group()\\n        normelized_abbreviation=abbreviation\\n        # normalize all \" \\n        if \"״\" in abbreviation:\\n          normelized_abbreviation=abbreviation.replace(\"״\",\"\"\")\\n        if \"\\'\\'\" in abbreviation:\\n          normelized_abbreviation=abbreviation.replace(\"\\'\\'\",\"\"\")\\n        # if its in the, replace with expanded form\\n        if normelized_abbreviation in ABBREVIATIONS:\\n            text = text.replace(abbreviation, ABBREVIATIONS[normelized_abbreviation])\\n    return text\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "TOKEN_PATTERN=\"[^ \\n]+\" # baseline tokenize, only by space and end of row\n",
        "DATASETS= [\"first\",\n",
        "           #\"second\",\n",
        "           #\"third\"\n",
        "           ]\n",
        "'''\n",
        "SPECIAL_CHARCATERS_REMOVAL=r\"(?u)\\b\\w+[\\\"\\'״]*\\w\\b\" # tokenize by \" \" and \"/n\" and allow abbreviations\n",
        "\n",
        "# Gets all the stop words from the file\n",
        "pattern = re.compile(SPECIAL_CHARCATERS_REMOVAL)\n",
        "with open(\"/content/drive/MyDrive/model_files/data/stop_words.txt\", 'r') as f:\n",
        "        STOP_WORDS = pattern.findall(f.read())\n",
        "\n",
        "# Insert all the abbreviations into a dictionary\n",
        "ABBREVIATIONS = {}\n",
        "with open(\"/content/drive/MyDrive/model_files/data/abbreviations.txt\") as f:\n",
        "    for line in f:\n",
        "        key, value = line.split(\"-\")\n",
        "        ABBREVIATIONS[key.strip()] = value.strip()\n",
        "\n",
        "# To preprocess open abbrevaitons\n",
        "def expand_abbreviations(text):\n",
        "    \"\"\"\n",
        "    a function that gets a text and replace all abbreviations with thier expnaded form\n",
        "    \"\"\"\n",
        "    for match in re.finditer(SPECIAL_CHARCATERS_REMOVAL, text):\n",
        "        abbreviation = match.group()\n",
        "        normelized_abbreviation=abbreviation\n",
        "        # normalize all \" \n",
        "        if \"״\" in abbreviation:\n",
        "          normelized_abbreviation=abbreviation.replace(\"״\",\"\\\"\")\n",
        "        if \"''\" in abbreviation:\n",
        "          normelized_abbreviation=abbreviation.replace(\"''\",\"\\\"\")\n",
        "        # if its in the, replace with expanded form\n",
        "        if normelized_abbreviation in ABBREVIATIONS:\n",
        "            text = text.replace(abbreviation, ABBREVIATIONS[normelized_abbreviation])\n",
        "    return text\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp35pG2B2a-k"
      },
      "outputs": [],
      "source": [
        "def read_train_text(dataset):\n",
        "  store = pd.HDFStore(f'/content/drive/MyDrive/suicide_model_files/data/{dataset}/train_text.h5')\n",
        "  df = store.select('dataset')\n",
        "  store.close()\n",
        "\n",
        "  columns_to_include = [\"text\"]\n",
        "  df = df[columns_to_include]\n",
        "  df.dropna(subset=[\"text\"], inplace=True)\n",
        "  return df\n",
        "def get_words(texts,preprocessing):\n",
        "  \"\"\"\n",
        "  returns all the tweets words by the preprocessing method\n",
        "  \"\"\"\n",
        "  corpus=texts\n",
        "  '''\n",
        "  if \"O\" in preprocessing:\n",
        "    corpus=corpus.apply(expand_abbreviations)\n",
        "  \n",
        "  if \"C\" in preprocessing:\n",
        "    pattern = re.compile(SPECIAL_CHARCATERS_REMOVAL)\n",
        "  else:\n",
        "  '''\n",
        "  pattern = re.compile(TOKEN_PATTERN)\n",
        "\n",
        "  corpus_words=[]\n",
        "  for document in tqdm(corpus):\n",
        "      corpus_words+= pattern.findall(document)\n",
        "  return corpus_words\n",
        "\n",
        "def get_terms():\n",
        "  columns= [\"nt\", \"f\", \"tf\",\"idf\",\"tfidf\"]\n",
        "  meanings= [\"Number of different documents that the word appears in.\",\n",
        "   \"Number of appearances of the word in all documents.\",\n",
        "   \"Term frequency.\",\n",
        "   \"Inverse document frequency.\",\n",
        "   \"Term fruquency multiplied by inverse document frequency.\"]\n",
        "  return columns, meanings\n",
        "   \n",
        "  \n",
        "def write_tf_idf_chart(preprocessing, vectorizer, dataset):\n",
        "  \"\"\"\n",
        "  writes the tf idf chart by the given preprocessing vectorizer\n",
        "  \"\"\"\n",
        "  train_texts = read_train_text(dataset)\n",
        "  sparse_matrix = vectorizer.fit_transform(train_texts[\"text\"])\n",
        "\n",
        "  # Get the vocabulary from the vectorizer object\n",
        "  vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "  df = pd.DataFrame(columns=['word', 'nt', 'f', 'tf', 'idf', 'tf-idf',' ','Column','Meaning'])\n",
        "  # Gather all the texts words to calculate words frequency\n",
        "  print(\"Getting words from texts:\")\n",
        "  texts_words = get_words(train_texts[\"text\"], preprocessing)\n",
        "  print(\"done!\")\n",
        "  print(len(vocab))\n",
        "\n",
        "  print(\"calculating 'nt' and 'f':\")\n",
        "  for i, word in tqdm(enumerate(vocab)):\n",
        "      # The column of tf-idf values of the specific word, as the rows are the texts \n",
        "      word_column = sparse_matrix.getcol(i)\n",
        "      # Count the number of texts the word appears in (get no zero = getnz)\n",
        "      nt = word_column.getnnz()\n",
        "      # Count the number of times the word is used in all texts\n",
        "      f = texts_words.count(word)\n",
        "      # append all the data we collected so far to each word to the data frame\n",
        "      new_df = pd.DataFrame({'word': word, 'nt': nt, 'f': f, 'tf': 0, 'idf': 0, 'tf-idf': 0}, index=[0])\n",
        "\n",
        "      df = pd.concat([df, new_df], ignore_index=True)\n",
        "  print(\"done!\")\n",
        "\n",
        "  # the number of words in total is the sum of the f of each word\n",
        "  sig_f = df['f'].sum()\n",
        "\n",
        "  print(\"calculating 'tf','idf' and 'tf-idf':\")\n",
        "  for i, word in tqdm(enumerate(vocab)):\n",
        "      # the index of the word\n",
        "      index = df.index[df['word'] == word][0]\n",
        "      f = df.at[index, 'f']\n",
        "      nt = df.at[index, 'nt']\n",
        "      # Calculate the tf value\n",
        "      tf = f / sig_f\n",
        "      # shape[0] is the number of rows, aka the number of texts\n",
        "      n = sparse_matrix.shape[0]\n",
        "      # Calculate the idf value\n",
        "      idf = np.log(n / (nt + 1))\n",
        "      # Calculate the tf-idf value\n",
        "      tf_idf = tf * idf\n",
        "      # update the word's remaining information to the dataframe\n",
        "      df.at[index, 'tf'] = tf\n",
        "      df.at[index, 'idf'] = idf\n",
        "      df.at[index, 'tf-idf'] = tf_idf\n",
        "  print(\"done!\")\n",
        "\n",
        "  # sort in descending order\n",
        "  df = df.sort_values('tf-idf', ascending=False)\n",
        "  # add vocabulary\n",
        "  columns, meanings = get_terms()\n",
        "  for i in range(len(columns)):\n",
        "    df.at[df.index[i], \"Column\"] = columns[i]\n",
        "    df.at[df.index[i], \"Meaning\"] = meanings[i]\n",
        "  # write to csv\n",
        "  df.to_csv(f'/content/drive/MyDrive/suicide_model_files/tf idf charts/{dataset}/{preprocessing}_words_tfidf_chart.csv', encoding='latin-1',index=False)\n",
        "  # write to h5\n",
        "  df.to_hdf(f'/content/drive/MyDrive/suicide_model_files/tf idf charts/{dataset}/{preprocessing}_words_tfidf_chart.h5', key='words_df', mode='w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_csIFg612eGw",
        "outputId": "7e770167-31cf-43d8-bddd-c622e68ee512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "writing charts for the first dataset.\n",
            "------------------------------------------\n",
            "writing the 5gram tf-idf chart:\n",
            "Getting words from texts:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14000/14000 [00:00<00:00, 32434.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done!\n",
            "60621\n",
            "calculating 'nt' and 'f':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "60621it [2:51:38,  5.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done!\n",
            "calculating 'tf','idf' and 'tf-idf':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "60621it [08:16, 122.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-7d5ee468499c>:104: PerformanceWarning: \n",
            "your performance may suffer as PyTables will pickle object types that it cannot\n",
            "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['word', 'nt', 'f', 'tf', 'idf', 'tf-idf', ' ', 'Column', 'Meaning'], dtype='object')]\n",
            "\n",
            "  df.to_hdf(f'/content/drive/MyDrive/suicide_model_files/tf idf charts/{dataset}/{preprocessing}_words_tfidf_chart.h5', key='words_df', mode='w')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for ds in DATASETS:\n",
        "  nt = 0\n",
        "  if ds == 'first':\n",
        "    nt = 10\n",
        "  if ds == 'second':\n",
        "    nt = 2\n",
        "  if ds == 'third':\n",
        "    nt = 3\n",
        "  VECTORIZERS={ # the tf-idf vectorizers by preprocessing method\n",
        "    #'N' : TfidfVectorizer(min_df=nt,lowercase=False, token_pattern=TOKEN_PATTERN),#None, baseline\n",
        "    #'3gram' : TfidfVectorizer(min_df=nt,lowercase=False, analyzer='char', ngram_range=(3, 3)),\n",
        "    #'4gram' : TfidfVectorizer(min_df=nt,lowercase=False, analyzer='char', ngram_range=(4, 4)),\n",
        "    '5gram' : TfidfVectorizer(min_df=nt,lowercase=False, analyzer='char', ngram_range=(5, 5)),\n",
        "\n",
        "    # only one preprocessing method\n",
        "    #'S':TfidfVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS),#Stop word removal\n",
        "    #'C':TfidfVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL),#special Characters removal\n",
        "    #'O':TfidfVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN, preprocessor=expand_abbreviations),#Open abbreviations\n",
        "    # pairing preprocssing methods\n",
        "    #'SC':TfidfVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS),\n",
        "    #'SO':TfidfVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "    #'CO':TfidfVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,preprocessor=expand_abbreviations),\n",
        "    # all preprocessing methods\n",
        "    #'SCO':TfidfVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "}\n",
        "  print(f'writing charts for the {ds} dataset.')\n",
        "  #iterate over each preprocessing method and writes the tfidf chart\n",
        "  for preprocessing, vectorizer in VECTORIZERS.items():\n",
        "    print(\"------------------------------------------\")\n",
        "    print(f\"writing the {preprocessing} tf-idf chart:\")\n",
        "    write_tf_idf_chart(preprocessing, vectorizer,ds)\n",
        "    print(\"------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cGdc5_xmuRy"
      },
      "source": [
        "###Vectorize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_udJorCzm7-1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "3b590abb-0fa6-42be-d4d9-3da09159f719"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nSPECIAL_CHARCATERS_REMOVAL=r\"(?u)\\x08\\\\w+[\"\\'״]*\\\\w\\x08\" # tokenize by \" \" and \"/n\" and allow abbreviations\\n\\n# Gets all the stop words from the file\\nwith open(\"/content/drive/MyDrive/model_files/data/stop_words.txt\", \\'r\\') as f:\\n        STOP_WORDS = f.read().splitlines()\\n\\n# Insert all the abbreviations into a dictionary\\nABBREVIATIONS = {}\\nwith open(\"/content/drive/MyDrive/model_files/data/abbreviations.txt\") as f:\\n    for line in f:\\n        key, value = line.split(\"-\")\\n        ABBREVIATIONS[key.strip()] = value.strip()\\n\\n# To preprocess open abbrevaitons\\ndef expand_abbreviations(text):\\n    \"\"\"\\n    a function that gets a text and replace all abbreviations with thier expnaded form\\n    \"\"\"\\n    for match in re.finditer(SPECIAL_CHARCATERS_REMOVAL, text):\\n        abbreviation = match.group()\\n        normelized_abbreviation=abbreviation\\n        # normalize all \" \\n        if \"״\" in abbreviation:\\n          normelized_abbreviation=abbreviation.replace(\"״\",\"\"\")\\n        if \"\\'\\'\" in abbreviation:\\n          normelized_abbreviation=abbreviation.replace(\"\\'\\'\",\"\"\")\\n        # if its in the, replace with expanded form\\n        if normelized_abbreviation in ABBREVIATIONS:\\n            text = text.replace(abbreviation, ABBREVIATIONS[normelized_abbreviation])\\n    return text\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "DATASETS= [\"first\" ,\n",
        "           #\"second\", \"third\"\n",
        "           ]\n",
        "NUMBER_OF_WORDS = [5000,4000,3000,2000,1000]  # the number of the highest tfidf words\n",
        "TOKEN_PATTERN=\"[^ \\n]+\" # baseline tokenize, only by space and end of row\n",
        "\n",
        "'''\n",
        "SPECIAL_CHARCATERS_REMOVAL=r\"(?u)\\b\\w+[\\\"\\'״]*\\w\\b\" # tokenize by \" \" and \"/n\" and allow abbreviations\n",
        "\n",
        "# Gets all the stop words from the file\n",
        "with open(\"/content/drive/MyDrive/model_files/data/stop_words.txt\", 'r') as f:\n",
        "        STOP_WORDS = f.read().splitlines()\n",
        "\n",
        "# Insert all the abbreviations into a dictionary\n",
        "ABBREVIATIONS = {}\n",
        "with open(\"/content/drive/MyDrive/model_files/data/abbreviations.txt\") as f:\n",
        "    for line in f:\n",
        "        key, value = line.split(\"-\")\n",
        "        ABBREVIATIONS[key.strip()] = value.strip()\n",
        "\n",
        "# To preprocess open abbrevaitons\n",
        "def expand_abbreviations(text):\n",
        "    \"\"\"\n",
        "    a function that gets a text and replace all abbreviations with thier expnaded form\n",
        "    \"\"\"\n",
        "    for match in re.finditer(SPECIAL_CHARCATERS_REMOVAL, text):\n",
        "        abbreviation = match.group()\n",
        "        normelized_abbreviation=abbreviation\n",
        "        # normalize all \" \n",
        "        if \"״\" in abbreviation:\n",
        "          normelized_abbreviation=abbreviation.replace(\"״\",\"\\\"\")\n",
        "        if \"''\" in abbreviation:\n",
        "          normelized_abbreviation=abbreviation.replace(\"''\",\"\\\"\")\n",
        "        # if its in the, replace with expanded form\n",
        "        if normelized_abbreviation in ABBREVIATIONS:\n",
        "            text = text.replace(abbreviation, ABBREVIATIONS[normelized_abbreviation])\n",
        "    return text\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSHt93yEnOMR"
      },
      "outputs": [],
      "source": [
        "def read_train_text(dataset):\n",
        "  store = pd.HDFStore(f'/content/drive/MyDrive/suicide_model_files/data/{dataset}/train_text.h5')\n",
        "  df = store.select('dataset')\n",
        "  store.close()\n",
        "\n",
        "  columns_to_include = [\"text\"]\n",
        "  df = df[columns_to_include]\n",
        "  df.dropna(subset=[\"text\"], inplace=True)\n",
        "  return df\n",
        "\n",
        "def read_test_text(dataset):\n",
        "  store = pd.HDFStore(f'/content/drive/MyDrive/suicide_model_files/data/{dataset}/test_text.h5')\n",
        "  df = store.select('dataset')\n",
        "  store.close()\n",
        "\n",
        "  columns_to_include = [\"text\"]\n",
        "  df = df[columns_to_include]\n",
        "  df.dropna(subset=[\"text\"], inplace=True)\n",
        "  return df\n",
        "\n",
        "\n",
        "def read_train_class(dataset):\n",
        "  \"\"\"\n",
        "  reads the data from the train knesst members sheet\n",
        "  :return: the data as pandas data set\n",
        "  \"\"\"\n",
        "  store = pd.HDFStore(f'/content/drive/MyDrive/suicide_model_files/data/{dataset}/train_class.h5')\n",
        "  df = store.select('dataset')\n",
        "  store.close()\n",
        "\n",
        "  columns_to_include = \"is_suicide\"\n",
        "  df = df[columns_to_include]\n",
        "  df.dropna( inplace=True)\n",
        "  return df\n",
        "\n",
        "def read_test_class(dataset):\n",
        "  \"\"\"\n",
        "  reads the data from the test knesst members sheet\n",
        "  :return: the data as pandas data set\n",
        "  \"\"\"\n",
        "  store = pd.HDFStore(f'/content/drive/MyDrive/suicide_model_files/data/{dataset}/test_class.h5')\n",
        "  df = store.select('dataset')\n",
        "  store.close()\n",
        "\n",
        "  columns_to_include = \"is_suicide\"\n",
        "  df = df[columns_to_include]\n",
        "  df.dropna( inplace=True)\n",
        "  return df\n",
        "\n",
        "def write_classification(dataset):\n",
        "  \"\"\"\n",
        "  return the tweet vector with the classification\n",
        "  :param tweets: the tweets set\n",
        "  :param knesset_members_train_set: the knesset members information\n",
        "  :param classification: the desired classification\n",
        "  :return: the classified tweets dataframe\n",
        "  \"\"\"\n",
        "  train_class=read_train_class(dataset)\n",
        "  test_class=read_test_class(dataset)\n",
        "\n",
        "  #df.to_csv(f'/content/drive/MyDrive/model_files/classification/{set}_tweets_classification_{classification}.csv', encoding='utf-8-sig', index=False)\n",
        "  train_class.to_hdf(f'/content/drive/MyDrive/suicide_model_files/classification/{dataset}/train_text_classification.h5', key='classification', mode='w')\n",
        "  test_class.to_hdf(f'/content/drive/MyDrive/suicide_model_files/classification/{dataset}/test_text_classification.h5', key='classification', mode='w')\n",
        "\n",
        "\n",
        "def write_vectors(tfidf_vocabulary,vectors,set,amount,preprocessing,dataset):\n",
        "  \"\"\"\n",
        "  write the given set tfidf vectors by the tf idf vocabulary\n",
        "  \"\"\"\n",
        "  dense_vectors = vectors.toarray()\n",
        "  df = pd.DataFrame(dense_vectors, columns=tfidf_vocabulary)\n",
        "  #df.to_csv(f'/content/drive/MyDrive/suicide_model_files/vectors/{dataset}/{preprocessing}_{set}_tweets_vectors_{amount}.csv', encoding='utf-8-sig',index=False)\n",
        "  df.to_hdf(f'/content/drive/MyDrive/suicide_model_files/vectors/{dataset}/{preprocessing}_{set}_text_vectors_{amount}.h5', key='vectors', mode='w')\n",
        "\n",
        "def get_words(amount,preprocessing,dataset):\n",
        "  \"\"\"\n",
        "  get the highest tf-idf words by a given amount from the df-idf chart\n",
        "  :param amount: the amount of words\n",
        "  :return: the given amount of words with the highest TF-IDF value\n",
        "  \"\"\"\n",
        "  words = pd.read_hdf(f'/content/drive/MyDrive/suicide_model_files/tf idf charts/{dataset}/{preprocessing}_words_tfidf_chart.h5',key=\"words_df\")\n",
        "  words = words.head(amount)\n",
        "  words = words['word'].tolist()\n",
        "  return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaqBWfmcnTkG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8edbe44d-e94f-4d4d-a663-b969d2192f49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 1/4 [01:08<03:25, 68.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3gram\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 2/4 [01:32<01:25, 42.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4gram\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 3/4 [01:57<00:34, 34.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5gram\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [02:23<00:00, 35.80s/it]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 1/4 [00:53<02:40, 53.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3gram\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 2/4 [01:15<01:09, 34.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4gram\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 3/4 [01:37<00:28, 28.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5gram\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [01:59<00:00, 29.97s/it]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 1/4 [00:37<01:51, 37.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3gram\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 2/4 [00:58<00:55, 27.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4gram\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 3/4 [01:18<00:24, 24.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5gram\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [01:40<00:00, 25.16s/it]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 1/4 [00:29<01:28, 29.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3gram\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 2/4 [00:45<00:43, 21.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4gram\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 3/4 [01:03<00:19, 19.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5gram\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [01:22<00:00, 20.59s/it]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 1/4 [00:19<00:57, 19.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3gram\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 2/4 [00:34<00:34, 17.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4gram\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 3/4 [00:49<00:16, 16.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5gram\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [01:04<00:00, 16.21s/it]\n"
          ]
        }
      ],
      "source": [
        "for ds in DATASETS:\n",
        "  print(ds)\n",
        "  train_texts = read_train_text(ds)\n",
        "  test_texts = read_test_text(ds)\n",
        "  for amount in NUMBER_OF_WORDS:\n",
        "    VECTORIZERS={ # the tf-idf vectorizers by preprocessing method\n",
        "      'N' : TfidfVectorizer(vocabulary=get_words(amount,'N',ds),lowercase=False, token_pattern=TOKEN_PATTERN),#None, baseline\n",
        "      '3gram' : TfidfVectorizer(vocabulary=get_words(amount,'3gram',ds),lowercase=False, analyzer='char', ngram_range=(3, 3)),\n",
        "      '4gram' : TfidfVectorizer(vocabulary=get_words(amount,'4gram',ds),lowercase=False, analyzer='char', ngram_range=(4, 4)),\n",
        "      '5gram' : TfidfVectorizer(vocabulary=get_words(amount,'5gram',ds),lowercase=False, analyzer='char', ngram_range=(5, 5)),\n",
        "\n",
        "    # only one preprocessing method\n",
        "      #'S':TfidfVectorizer(vocabulary=get_words(amount,'S'),lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS),#Stop word removal\n",
        "      #'C':TfidfVectorizer(vocabulary=get_words(amount,'C'),lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL),#special Characters removal\n",
        "      #'O':TfidfVectorizer(vocabulary=get_words(amount,'O'),lowercase=False, token_pattern=TOKEN_PATTERN, preprocessor=expand_abbreviations),#Open abbreviations\n",
        "    # pairing preprocssing methods\n",
        "    #'SC':TfidfVectorizer(vocabulary=get_words(amount,'SC'),lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS),\n",
        "    #'SO':TfidfVectorizer(vocabulary=get_words(amount,'SO'),lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "    #'CO':TfidfVectorizer(vocabulary=get_words(amount,'CO'),lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,preprocessor=expand_abbreviations),\n",
        "    # all preprocessing methods\n",
        "    #'SCO':TfidfVectorizer(vocabulary=get_words(amount,'SCO'),lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "  }\n",
        "    for preprocessing, vectorizer in tqdm(VECTORIZERS.items()):\n",
        "      print(preprocessing)\n",
        "      tfidf_vocabulary=get_words(amount,preprocessing,ds)\n",
        "      train_vectors = vectorizer.fit_transform(train_texts[\"text\"])\n",
        "      write_vectors(tfidf_vocabulary,train_vectors,\"train\",amount,preprocessing,ds)\n",
        "\n",
        "      test_vectors = vectorizer.fit_transform(test_texts[\"text\"])\n",
        "      write_vectors(tfidf_vocabulary,test_vectors,\"test\",amount,preprocessing,ds)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ds in DATASETS:\n",
        "    write_classification(ds)   \n",
        "\n"
      ],
      "metadata": {
        "id": "0lGeKYS9hgCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD8NQMI8uuFb"
      },
      "source": [
        "##ML Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoOnv6hluwOK"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 1 # in order to get consistent results for the random forest algorithem\n",
        "DATASETS= [\"first\" ,\"second\", \"third\"]\n",
        "\n",
        "NUM_OF_WORDS=[1000,2000,3000,4000,5000]\n",
        "ML_CLASSIFIERS= [ # the classifers we use\n",
        "\"RF\"\n",
        ",\"SVM\"\n",
        ",\"MNB\"\n",
        ",\"LR\"\n",
        ",\"MLP\"\n",
        "]\n",
        "OVERSAMPLERS=[\"none\",\n",
        "              #\"random\",\n",
        "              #\"smote\",\n",
        "              #\"adasyn\"\n",
        "              ]\n",
        "# the preprocessing methods\n",
        "PREPROCESSORS=[#'N',\n",
        "               '3gram',\n",
        "               #'4gram',\n",
        "               #'5gram',\n",
        "]\n",
        "               #,'S','C','O','SC','SO','CO','SCO']\n",
        "RANDOM_STATE=42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-5PqzJUvBtI"
      },
      "outputs": [],
      "source": [
        "def get_sets_vectors(amount,preprocessing,oversampler,dataset):\n",
        "  \"\"\"\n",
        "  reads and returns the vectors and classifcation of the train and test sets.\n",
        "  \"\"\"\n",
        "  x_train=pd.read_hdf(f'/content/drive/MyDrive/suicide_model_files/vectors/{dataset}/{preprocessing}_train_text_vectors_{amount}.h5',key='vectors')\n",
        "  x_train.columns = x_train.columns.astype(str)\n",
        "  y_train=pd.read_hdf(f'/content/drive/MyDrive/suicide_model_files/classification/{dataset}/train_text_classification.h5',key='classification')\n",
        "  y_train=np.array(y_train).astype(int)  \n",
        "  x_test=pd.read_hdf(f'/content/drive/MyDrive/suicide_model_files/vectors/{dataset}/{preprocessing}_test_text_vectors_{amount}.h5',key='vectors')\n",
        "  x_test.columns = x_test.columns.astype(str)\n",
        "  y_test=pd.read_hdf(f'/content/drive/MyDrive/suicide_model_files/classification/{dataset}/test_text_classification.h5',key='classification')\n",
        "  y_test=np.array(y_test).astype(int)\n",
        "\n",
        "\n",
        "  if oversampler != \"none\":\n",
        "    if oversampler == \"random\":\n",
        "      oversampler = RandomOverSampler(random_state=RANDOM_STATE)\n",
        "    if oversampler == \"smote\":\n",
        "      oversampler = SMOTE(random_state=RANDOM_STATE)\n",
        "    if oversampler == \"adasyn\":\n",
        "      oversampler = ADASYN(random_state=RANDOM_STATE)\n",
        "\n",
        "    x_train, y_train = oversampler.fit_resample(x_train, y_train)      \n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "\n",
        "def score_classifier(y_test, predictions, classifier,classifiers_scores):\n",
        "  \"\"\"\n",
        "  returns the given classifers score by comparing the predictions and true classifies.\n",
        "  \"\"\"\n",
        "  accuracy = accuracy_score(y_test, predictions)\n",
        "  classifiers_scores[classifier] = accuracy\n",
        "\n",
        "\n",
        "def run_classifier(x_train, x_test, y_train, y_test,classifier,classifiers_scores):\n",
        "  \"\"\"\n",
        "  runs the specified classifier with the given train and test sets.\n",
        "  \"\"\"\n",
        "  # create a scikit classifier by the given classifier name\n",
        "  if classifier==\"RF\":\n",
        "      clf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "  elif classifier==\"SVM\":\n",
        "      clf = SVC()\n",
        "  elif classifier==\"MLP\":\n",
        "      clf = MLPClassifier()\n",
        "  elif classifier==\"MNB\":\n",
        "      clf = MultinomialNB()\n",
        "  elif classifier==\"LR\":\n",
        "      clf = LogisticRegression()\n",
        "  else:\n",
        "      raise ValueError(\"unknown classifier given\")\n",
        "  clf.fit(x_train, y_train)\n",
        "  predictions = clf.predict(x_test)\n",
        "  score_classifier(y_test, predictions, classifier,classifiers_scores)\n",
        "  return clf\n",
        "\n",
        "\n",
        "def ml_classifiers(x_train, x_test, y_train, y_test,classifiers):\n",
        "  \"\"\"\n",
        "  creates the classifers and prints their scores. \n",
        "  \"\"\"\n",
        "  # scores dictionaries to store and summary by measure\n",
        "  classifiers_scores = {}\n",
        "  for classifier in tqdm(classifiers):\n",
        "    run_classifier(x_train, x_test, y_train, y_test,classifier,classifiers_scores)\n",
        "\n",
        "  return classifiers_scores\n",
        "\n",
        "def create_score_chart(oversampling,preprocessing,dataset):\n",
        "  df_columns = []\n",
        "  # Create a data frame with the title row\n",
        "  df_columns.append(\"Number of words\")\n",
        "  df_columns = df_columns + ML_CLASSIFIERS\n",
        "  df = pd.DataFrame(columns=df_columns)\n",
        "    \n",
        "  # Create an array of empty strings\n",
        "  empty_strings = np.empty(len(df_columns), dtype='U64')\n",
        "  empty_strings[:] = ''\n",
        "\n",
        "  # Set the first element of the array to a specific value\n",
        "  empty_strings[0] = oversampling+ \" oversampling after preprocess by \" + preprocessing\n",
        "  title = pd.DataFrame(data=[empty_strings], columns=df.columns)\n",
        "  for amount in NUM_OF_WORDS:\n",
        "      print(f\"running ml methods for {amount} of words with preprocessing '{preprocessing}'...\")\n",
        "      x_train, x_test, y_train, y_test=get_sets_vectors(amount,preprocessing,oversampling,dataset)\n",
        "      classifiers_scores = ml_classifiers(x_train, x_test, y_train, y_test,ML_CLASSIFIERS)\n",
        "      print(classifiers_scores)\n",
        "      df_row = []\n",
        "      df_row.append(amount)\n",
        "      df_row = df_row + list(classifiers_scores.values())\n",
        "      df.loc[len(df.index)] = df_row\n",
        "  print(tabulate(df, headers='keys', tablefmt='psql'))\n",
        "  # Concatenate the two data frames\n",
        "  print(oversampling+\" \"+preprocessing+\":\")\n",
        "  header = pd.DataFrame(data=[df_columns], columns=df_columns)\n",
        "  csv_df = pd.concat([title,header,df], ignore_index=True)\n",
        "  csv_df.to_csv(f'/content/drive/MyDrive/suicide_model_files/results/{dataset}/{oversampling}_oversampling_ of_{preprocessing}_results.csv', header=False, encoding='utf-8-sig', index=False)\n",
        "  df.to_hdf(f'/content/drive/MyDrive/suicide_model_files/results/{dataset}/{oversampling}_oversampling_ of_{preprocessing}_results.h5', key='results', mode='w')\n",
        "  print(\"finished writing \"+oversampling+\" oversampling by \"+preprocessing+\":\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6V43VgWvCgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e2417d-6893-4c35-f323-7b9a8088d641"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running ml methods for 1000 of words with preprocessing '3gram'...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF\n",
            "0.6133567662565905\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:00<00:02,  1.50it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM\n",
            "0.6379613356766256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [00:01<00:02,  1.36it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MNB\n",
            "0.632688927943761\n",
            "LR\n",
            "0.6344463971880492\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [00:01<00:00,  3.19it/s]/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP\n",
            "0.5764499121265377\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:10<00:00,  2.09s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'RF': 0.6133567662565905, 'SVM': 0.6379613356766256, 'MNB': 0.632688927943761, 'LR': 0.6344463971880492, 'MLP': 0.5764499121265377}\n",
            "running ml methods for 2000 of words with preprocessing '3gram'...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF\n",
            "0.671353251318102\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:00<00:03,  1.05it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM\n",
            "0.664323374340949\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [00:03<00:05,  1.98s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MNB\n",
            "0.6678383128295254\n",
            "LR\n",
            "0.671353251318102\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [00:03<00:00,  1.24it/s]/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP\n",
            "0.6274165202108963\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:20<00:00,  4.16s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'RF': 0.671353251318102, 'SVM': 0.664323374340949, 'MNB': 0.6678383128295254, 'LR': 0.671353251318102, 'MLP': 0.6274165202108963}\n",
            "running ml methods for 3000 of words with preprocessing '3gram'...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF\n",
            "0.6502636203866432\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:01<00:04,  1.17s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM\n",
            "0.6695957820738138\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [00:05<00:08,  2.86s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MNB\n",
            "0.6660808435852372\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 3/5 [00:05<00:03,  1.61s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR\n",
            "0.664323374340949\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [00:05<00:01,  1.13s/it]/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP\n",
            "0.6274165202108963\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:28<00:00,  5.78s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'RF': 0.6502636203866432, 'SVM': 0.6695957820738138, 'MNB': 0.6660808435852372, 'LR': 0.664323374340949, 'MLP': 0.6274165202108963}\n",
            "running ml methods for 4000 of words with preprocessing '3gram'...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF\n",
            "0.6414762741652021\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:01<00:05,  1.32s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM\n",
            "0.6748681898066784\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [00:06<00:10,  3.53s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MNB\n",
            "0.6818980667838312\n",
            "LR\n",
            "0.6766256590509666\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [00:06<00:01,  1.42s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP\n",
            "0.6502636203866432\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:38<00:00,  7.67s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'RF': 0.6414762741652021, 'SVM': 0.6748681898066784, 'MNB': 0.6818980667838312, 'LR': 0.6766256590509666, 'MLP': 0.6502636203866432}\n",
            "running ml methods for 5000 of words with preprocessing '3gram'...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF\n",
            "0.6590509666080844\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:01<00:05,  1.48s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM\n",
            "0.671353251318102\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [00:08<00:13,  4.65s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MNB\n",
            "0.671353251318102\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 3/5 [00:08<00:05,  2.57s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR\n",
            "0.671353251318102\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [00:08<00:01,  1.69s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP\n",
            "0.6362038664323374\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:43<00:00,  8.74s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'RF': 0.6590509666080844, 'SVM': 0.671353251318102, 'MNB': 0.671353251318102, 'LR': 0.671353251318102, 'MLP': 0.6362038664323374}\n",
            "+----+-------------------+----------+----------+----------+----------+----------+\n",
            "|    |   Number of words |       RF |      SVM |      MNB |       LR |      MLP |\n",
            "|----+-------------------+----------+----------+----------+----------+----------|\n",
            "|  0 |              1000 | 0.613357 | 0.637961 | 0.632689 | 0.634446 | 0.57645  |\n",
            "|  1 |              2000 | 0.671353 | 0.664323 | 0.667838 | 0.671353 | 0.627417 |\n",
            "|  2 |              3000 | 0.650264 | 0.669596 | 0.666081 | 0.664323 | 0.627417 |\n",
            "|  3 |              4000 | 0.641476 | 0.674868 | 0.681898 | 0.676626 | 0.650264 |\n",
            "|  4 |              5000 | 0.659051 | 0.671353 | 0.671353 | 0.671353 | 0.636204 |\n",
            "+----+-------------------+----------+----------+----------+----------+----------+\n",
            "none 3gram:\n",
            "finished writing none oversampling by 3gram:\n",
            "running ml methods for 1000 of words with preprocessing '3gram'...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF\n",
            "0.8874269005847953\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:03<00:13,  3.36s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM\n",
            "0.9108187134502924\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [00:19<00:32, 10.93s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MNB\n",
            "0.8267543859649122\n",
            "LR\n",
            "0.8881578947368421\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [00:19<00:04,  4.17s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP\n",
            "0.8855994152046783\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [01:00<00:00, 12.06s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'RF': 0.8874269005847953, 'SVM': 0.9108187134502924, 'MNB': 0.8267543859649122, 'LR': 0.8881578947368421, 'MLP': 0.8855994152046783}\n",
            "running ml methods for 2000 of words with preprocessing '3gram'...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF\n",
            "0.893640350877193\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:04<00:18,  4.53s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM\n",
            "0.9203216374269005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [00:33<00:57, 19.11s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MNB\n",
            "0.8486842105263158\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 3/5 [00:33<00:20, 10.45s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR\n",
            "0.8961988304093568\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [00:34<00:06,  6.56s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP\n",
            "0.8881578947368421\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [01:28<00:00, 17.74s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'RF': 0.893640350877193, 'SVM': 0.9203216374269005, 'MNB': 0.8486842105263158, 'LR': 0.8961988304093568, 'MLP': 0.8881578947368421}\n",
            "running ml methods for 3000 of words with preprocessing '3gram'...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF\n",
            "0.903874269005848\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:05<00:23,  5.80s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM\n",
            "0.918859649122807\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [00:50<01:25, 28.45s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MNB\n",
            "0.8559941520467836\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 3/5 [00:50<00:31, 15.55s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR\n",
            "0.9064327485380117\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [00:51<00:09,  9.79s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP\n",
            "0.8969298245614035\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [01:53<00:00, 22.70s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'RF': 0.903874269005848, 'SVM': 0.918859649122807, 'MNB': 0.8559941520467836, 'LR': 0.9064327485380117, 'MLP': 0.8969298245614035}\n",
            "running ml methods for 4000 of words with preprocessing '3gram'...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF\n",
            "0.9024122807017544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 1/5 [00:06<00:26,  6.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM\n",
            "0.9250730994152047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [01:04<01:50, 36.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNB\n",
            "0.8676900584795322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [01:04<00:40, 20.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR\n",
            "0.9108187134502924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [01:05<00:12, 12.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP\n",
            "0.8969298245614035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [02:15<00:00, 27.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'RF': 0.9024122807017544, 'SVM': 0.9250730994152047, 'MNB': 0.8676900584795322, 'LR': 0.9108187134502924, 'MLP': 0.8969298245614035}\n",
            "running ml methods for 5000 of words with preprocessing '3gram'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF\n",
            "0.9093567251461988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 1/5 [00:06<00:27,  6.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM\n",
            "0.9301900584795322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [01:16<02:11, 43.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNB\n",
            "0.8676900584795322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [01:16<00:47, 23.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR\n",
            "0.9133771929824561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [01:17<00:14, 14.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP\n",
            "0.9057017543859649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [02:33<00:00, 30.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'RF': 0.9093567251461988, 'SVM': 0.9301900584795322, 'MNB': 0.8676900584795322, 'LR': 0.9133771929824561, 'MLP': 0.9057017543859649}\n",
            "+----+-------------------+----------+----------+----------+----------+----------+\n",
            "|    |   Number of words |       RF |      SVM |      MNB |       LR |      MLP |\n",
            "|----+-------------------+----------+----------+----------+----------+----------|\n",
            "|  0 |              1000 | 0.887427 | 0.910819 | 0.826754 | 0.888158 | 0.885599 |\n",
            "|  1 |              2000 | 0.89364  | 0.920322 | 0.848684 | 0.896199 | 0.888158 |\n",
            "|  2 |              3000 | 0.903874 | 0.91886  | 0.855994 | 0.906433 | 0.89693  |\n",
            "|  3 |              4000 | 0.902412 | 0.925073 | 0.86769  | 0.910819 | 0.89693  |\n",
            "|  4 |              5000 | 0.909357 | 0.93019  | 0.86769  | 0.913377 | 0.905702 |\n",
            "+----+-------------------+----------+----------+----------+----------+----------+\n",
            "none 3gram:\n",
            "finished writing none oversampling by 3gram:\n"
          ]
        }
      ],
      "source": [
        "for ds in DATASETS:\n",
        "  for oversampling in OVERSAMPLERS:\n",
        "    processes = [] #open a process for each preprocessing method\n",
        "    for preprocessing in PREPROCESSORS:\n",
        "      p = Process(target=create_score_chart, args=(oversampling,preprocessing,ds))\n",
        "      p.start()\n",
        "      processes.append(p)\n",
        "\n",
        "    # wait for all processes to finish\n",
        "    for p in processes:\n",
        "      p.join()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Summary"
      ],
      "metadata": {
        "id": "i_7mdhjDEoZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASETS= [\"first\" ,\"second\", \"third\"]\n",
        "PREPROCESSORS=['N',\n",
        "               '3gram',\n",
        "               '4gram',\n",
        "               '5gram'\n",
        "               #'S','C','O'\n",
        "               ]#,'SC','SO','CO','SCO']\n",
        "ML_CLASSIFIERS= [ # the classifers we use\n",
        "\"RF\"\n",
        ",\"SVM\"\n",
        ",\"MNB\"\n",
        ",\"LR\"\n",
        ",\"MLP\"\n",
        "]\n",
        "NUM_OF_WORDS=[1000,2000,3000,4000,5000]\n",
        "\n",
        "OVERSAMPLERS=[\"none\",\n",
        "              #\"random\",\n",
        "              #\"smote\",\n",
        "              #\"adasyn\"\n",
        "              ]"
      ],
      "metadata": {
        "id": "h4Ps6VP3E9jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_columns = [\"Feature type\",\"Classifier\",\"Number of features\",\"Score\"]\n",
        "for oversampling in OVERSAMPLERS:\n",
        "  for ds in DATASETS:\n",
        "    print(f\"starting {ds} dataset:\")\n",
        "    summary = pd.DataFrame(columns=df_columns)\n",
        "    for preprocessing in PREPROCESSORS:\n",
        "        ml_results = pd.read_hdf(f'/content/drive/MyDrive/suicide_model_files/results/{ds}/{oversampling}_oversampling_ of_{preprocessing}_results.h5', key='results')\n",
        "        max_value_index = ml_results[ml_results.columns[1:]].stack().idxmax()\n",
        "        row_index= int(max_value_index[0])\n",
        "        column_index= ml_results.columns.get_loc(max_value_index[1])\n",
        "        row={\"Feature type\": preprocessing,\"Classifier\": max_value_index[1],\"Number of features\": ml_results.iat[row_index,0],\"Score\":ml_results.iat[row_index,column_index] }\n",
        "        summary.loc[len(summary.index)] = row\n",
        "    summary = summary.sort_values('Score', ascending=False)\n",
        "    print(tabulate(summary, headers='keys', tablefmt='psql',showindex=False))\n",
        "    summary.to_csv(f'/content/drive/MyDrive/suicide_model_files/summary/datasets/{ds}/results_summary.csv', index=False)\n",
        "    summary.to_hdf(f'/content/drive/MyDrive/suicide_model_files/summary/datasets/{ds}/results_summary.h5', key='summary', mode='w') \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMhtp5f2Es4v",
        "outputId": "a1c26896-8af6-4d7f-fe9c-5896f291a4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting first dataset:\n",
            "+----------------+--------------+----------------------+----------+\n",
            "| Feature type   | Classifier   |   Number of features |    Score |\n",
            "|----------------+--------------+----------------------+----------|\n",
            "| 3gram          | SVM          |                 5000 | 0.9245   |\n",
            "| N              | SVM          |                 4000 | 0.918333 |\n",
            "| 4gram          | SVM          |                 5000 | 0.910833 |\n",
            "| 5gram          | SVM          |                 5000 | 0.894667 |\n",
            "+----------------+--------------+----------------------+----------+\n",
            "starting second dataset:\n",
            "+----------------+--------------+----------------------+----------+\n",
            "| Feature type   | Classifier   |   Number of features |    Score |\n",
            "|----------------+--------------+----------------------+----------|\n",
            "| 4gram          | SVM          |                 1000 | 0.688928 |\n",
            "| N              | MNB          |                 5000 | 0.683656 |\n",
            "| 3gram          | MNB          |                 4000 | 0.681898 |\n",
            "| 5gram          | SVM          |                 4000 | 0.669596 |\n",
            "+----------------+--------------+----------------------+----------+\n",
            "starting third dataset:\n",
            "+----------------+--------------+----------------------+----------+\n",
            "| Feature type   | Classifier   |   Number of features |    Score |\n",
            "|----------------+--------------+----------------------+----------|\n",
            "| 3gram          | SVM          |                 5000 | 0.93019  |\n",
            "| N              | SVM          |                 4000 | 0.927997 |\n",
            "| 4gram          | SVM          |                 4000 | 0.919956 |\n",
            "| 5gram          | SVM          |                 5000 | 0.899123 |\n",
            "+----------------+--------------+----------------------+----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_columns = [\"Dataset\",\"Feature type\",\"Classifier\",\"Number of features\",\"Score\"]\n",
        "oversampling = 'none'\n",
        "model_summary = pd.DataFrame(columns=df_columns)\n",
        "for ds in DATASETS:\n",
        "      summary= pd.read_hdf(f'/content/drive/MyDrive/suicide_model_files/summary/datasets/{ds}/results_summary.h5', key='summary')\n",
        "      summary[\"Dataset\"] = ds\n",
        "      model_summary = pd.concat([model_summary,summary], ignore_index=True)\n",
        "\n",
        "model_summary = model_summary.sort_values('Score', ascending=False)\n",
        "model_summary = model_summary.head(4)\n",
        "print(tabulate(model_summary, headers='keys', tablefmt='psql',showindex=False))\n",
        "model_summary.to_hdf(f'/content/drive/MyDrive/suicide_model_files/summary/baseline_results_summary.h5',key='model_summary', mode='w')\n",
        "model_summary.to_csv(f'/content/drive/MyDrive/suicide_model_files/summary/baseline_results_summary.csv', encoding='utf-8-sig', index=False)\n",
        "\n",
        "\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44iTYBK5Jb6R",
        "outputId": "d4cc1101-5bb4-4873-dfe1-bbf7406f1bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+--------------+----------------------+----------+\n",
            "| Dataset   | Feature type   | Classifier   |   Number of features |    Score |\n",
            "|-----------+----------------+--------------+----------------------+----------|\n",
            "| third     | 3gram          | SVM          |                 5000 | 0.93019  |\n",
            "| third     | N              | SVM          |                 4000 | 0.927997 |\n",
            "| third     | 4gram          | SVM          |                 4000 | 0.919956 |\n",
            "| third     | 5gram          | SVM          |                 5000 | 0.899123 |\n",
            "+-----------+----------------+--------------+----------------------+----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tuning"
      ],
      "metadata": {
        "id": "r7E9KQTrepZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_words(amount,preprocessing,dataset):\n",
        "  \"\"\"\n",
        "  get the highest tf-idf words by a given amount from the df-idf chart\n",
        "  :param amount: the amount of words\n",
        "  :return: the given amount of words with the highest TF-IDF value\n",
        "  \"\"\"\n",
        "  words = pd.read_hdf(f'/content/drive/MyDrive/suicide_model_files/tf idf charts/{dataset}/{preprocessing}_words_tfidf_chart.h5',key=\"words_df\")\n",
        "  words = words.head(amount)\n",
        "  words = words['word'].tolist()\n",
        "  return words\n",
        "\n",
        "model_summary = pd.read_hdf(f'/content/drive/MyDrive/suicide_model_files/summary/baseline_results_summary.h5', key='model_summary')\n",
        "\n",
        "amount = int(model_summary.iloc[0]['Number of features'])\n",
        "preprocessing = model_summary.iloc[0]['Feature type']\n",
        "oversampling = 'none'\n",
        "ds = model_summary.iloc[0]['Dataset']\n",
        "ml_method = model_summary.iloc[0]['Classifier']\n",
        "score = model_summary.iloc[0]['Score']\n",
        "print('start training:')\n",
        "print('amount: ' + str(amount))\n",
        "print('features type: '+ str(preprocessing))\n",
        "print('oversampling: '+ str(oversampling))\n",
        "print('dataset: '+ str(ds))\n",
        "print('ml method: '+ str(ml_method))\n",
        "print('score before tuning: ' +str(score) )\n",
        "\n",
        "x_train, x_test, y_train, y_test=get_sets_vectors(amount,preprocessing,oversampling,ds)\n",
        "\n",
        "TOKEN_PATTERN=\"[^ \\n]+\" # baseline tokenize, only by space and end of row\n",
        "VECTORIZERS={ # the tf-idf vectorizers by preprocessing method\n",
        "      'N' : TfidfVectorizer(vocabulary=get_words(amount,'N',ds),lowercase=False, token_pattern=TOKEN_PATTERN),#None, baseline\n",
        "      '3gram' : TfidfVectorizer(vocabulary=get_words(amount,'3gram',ds),lowercase=False, analyzer='char', ngram_range=(3, 3)),\n",
        "      '4gram' : TfidfVectorizer(vocabulary=get_words(amount,'4gram',ds),lowercase=False, analyzer='char', ngram_range=(4, 4)),\n",
        "      '5gram' : TfidfVectorizer(vocabulary=get_words(amount,'5gram',ds),lowercase=False, analyzer='char', ngram_range=(5, 5)),\n",
        "}\n",
        "vectorizer = VECTORIZERS[preprocessing]\n",
        "vectorizer.fit_transform(x_train)\n",
        "\n",
        "param_grid={}\n",
        "if ml_method == \"SVM\":\n",
        "  clf = SVC()\n",
        "  #clf.fit(x_train,y_train)\n",
        "  param_grid = {'C': [1], 'kernel': ['linear', 'poly','rbf'], 'degree': [2, 3, 4],'gamma': [10, 100, 1000]}\n",
        "\n",
        "x = pd.concat([x_train,x_test], axis = 0)\n",
        "y = np.concatenate([ y_train, y_test])\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(x, y)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "print('tuned params:')\n",
        "print(best_params)\n",
        "print('best score:')\n",
        "print(best_score)\n",
        "\n",
        "\n",
        "\n",
        "pickle.dump(grid_search, open('/content/drive/MyDrive/suicide_model_files/model.sav', 'wb'))\n",
        "pickle.dump(vectorizer, open('/content/drive/MyDrive/suicide_model_files/vectorizer.sav', 'wb'))\n",
        "\n",
        "print('done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qMAeTvbQkxE",
        "outputId": "ef292a9b-6629-4ee1-fb7e-5808e51b249a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start training:\n",
            "amount: 5000\n",
            "features type: 3gram\n",
            "oversampling: none\n",
            "dataset: third\n",
            "ml method: SVM\n",
            "score before tuning: 0.9301900584795322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Usage"
      ],
      "metadata": {
        "id": "i3o2FsB5dAPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('booting server.')\n",
        "model = pickle.load(open('/content/drive/MyDrive/suicide_model_files/model.sav', 'rb'))\n",
        "vectorizer = pickle.load(open('/content/drive/MyDrive/suicide_model_files/vectorizer.sav', 'rb'))\n",
        "user_input = \"\"\n",
        "print('server is up.')\n",
        "while True:\n",
        "  user_input = input(\"Please enter an input, type EXIT to leave:\")\n",
        "  if user_input == 'EXIT':\n",
        "    break\n",
        "  input_vector = vectorizer.transform([user_input]).toarray()\n",
        "  prediction = model.predict(input_vector)\n",
        "  if(prediction[0]):\n",
        "    print(\"NO!!!!!! you are still young\")\n",
        "  else:\n",
        "    print(\"yeeeeee, idc\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HNDf_DFdEd_",
        "outputId": "a4d6a725-dd17-4973-f135-b969de6fe874"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "booting server.\n",
            "server is up.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NO!!!!!! you are still young\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yeeeeee, idc\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NO!!!!!! you are still young\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}