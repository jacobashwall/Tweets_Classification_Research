{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobashwall/Battlegrounds/blob/main/tweets_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IGVkJQxVtxb"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB4MAZ3iL2Fp"
      },
      "source": [
        "##Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRWwmNDZNwMr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import h5py\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tabulate import tabulate\n",
        "from google.colab import files\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "import psutil\n",
        "import fcntl\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from multiprocessing import Process\n",
        "import tensorflow as tf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh8QpjvP-4GO",
        "outputId": "8ec4dd98-45cf-491e-afb4-3c9949e0550c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "/usr/bin/xdg-open: 869: www-browser: not found\n",
            "/usr/bin/xdg-open: 869: links2: not found\n",
            "/usr/bin/xdg-open: 869: elinks: not found\n",
            "/usr/bin/xdg-open: 869: links: not found\n",
            "/usr/bin/xdg-open: 869: lynx: not found\n",
            "/usr/bin/xdg-open: 869: w3m: not found\n",
            "xdg-open: no method available for opening 'https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=EG%2FFTqNfAi6TC1Rfwhy%2FDW5ZqyR5Q8LmXZYBMmMeEYQ'\n",
            "/bin/sh: 1: firefox: not found\n",
            "/bin/sh: 1: google-chrome: not found\n",
            "/bin/sh: 1: chromium-browser: not found\n",
            "/bin/sh: 1: open: not found\n",
            "Cannot retrieve auth tokens.\n",
            "Failure(\"Error opening URL:https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=EG%2FFTqNfAi6TC1Rfwhy%2FDW5ZqyR5Q8LmXZYBMmMeEYQ\")\n"
          ]
        }
      ],
      "source": [
        "!sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!sudo apt-get update -qq 2>&1 > /dev/null\n",
        "!sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n",
        "!google-drive-ocamlfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdvWIzkj_TTC",
        "outputId": "9177003f-8ac6-4a74-cc58-3e41d29d54b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package w3m.\n",
            "(Reading database ... 128280 files and directories currently installed.)\n",
            "Preparing to unpack .../w3m_0.5.3-37ubuntu0.1_amd64.deb ...\n",
            "Unpacking w3m (0.5.3-37ubuntu0.1) ...\n",
            "Setting up w3m (0.5.3-37ubuntu0.1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n",
            "/content\n",
            "/content/drive\n",
            "/content\n",
            "/\n",
            "Access token retrieved correctly.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -qq w3m # to act as web browser \n",
        "!xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT-cCLL1JUVJ"
      },
      "source": [
        "##Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjEG4LXyHTuq"
      },
      "source": [
        "###Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvBCJlIlEJYJ"
      },
      "outputs": [],
      "source": [
        "TRAIN_PERCENTAGE = 75  # the percentage to divide the dataset to training and testing datasets\n",
        "READ_COLUMNS=[ #the columns that are read from the csv file\n",
        "\"מס סידורי\"\n",
        ",\"שם משתמש\"\n",
        ",\"גיל\"\n",
        ",\"קבוצת גיל\"\n",
        ",\"מין\"\n",
        ",\"0-ימין\"\n",
        ",\"1-ימין מרכז\"\n",
        ",\"2-שמאל מרכז\"\n",
        ",\"3-שמאל\"\n",
        ",\"0-ליכוד\"\n",
        ",\"1-יש עתיד\"\n",
        ",\"2-הציונות הדתית\"\n",
        ",\"3-מרצ\"\n",
        ",\"4-תקווה חדשה\"\n",
        ",\"5-ימינה\"\n",
        ",\"6- ישראל ביתנו\"\n",
        ",\"7-כחול לבן\"\n",
        ",\"8-ש\\\"ס\"\n",
        ",\"9-הרשימה המשותפת\"\n",
        ",\"10-העבודה\"\n",
        ",\"11-רע\\\"מ\"\n",
        ",\"12-ח\\\"כ בודד\"\n",
        ",\"0-ספרדי\"\n",
        ",\"1-אשכנזי\"\n",
        ",\"דתי אורתודוקסי\"\n",
        ",\"לא חרדי-0\"\n",
        ",\"חרדי-1\"\n",
        ",\"יהודי\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeyLpHP9D_hO"
      },
      "outputs": [],
      "source": [
        "def read_tweets():\n",
        "  \"\"\"\n",
        "  reads the data from the tweets sheet\n",
        "  :return: the data as pandas data set\n",
        "  \"\"\"\n",
        "  return pd.read_csv('/content/drive/MyDrive/model_files/data/TweetsRes - The 24 Knesset_ Tweets.csv', encoding='utf-8-sig')\n",
        "\n",
        "\n",
        "def read_knesset_members():\n",
        "  \"\"\"\n",
        "  reads the data from the knesset members sheet\n",
        "  :return: the data as pandas data set\n",
        "  \"\"\"\n",
        "  return pd.read_csv('/content/drive/MyDrive/model_files/data/TweetsRes - The 24 Knesset_ information.csv', usecols= READ_COLUMNS, encoding='UTF8')\n",
        "\n",
        "def write_train_tweets(train_tweets_set):\n",
        "  \"\"\"\n",
        "  writes the tweets of the training tweets dataset\n",
        "  \"\"\"\n",
        "  train_tweets_set.to_csv('/content/drive/MyDrive/model_files/data/train_tweets_set.csv', encoding='utf-8-sig',index=False)\n",
        "  train_tweets_set.to_hdf('/content/drive/MyDrive/model_files/data/train_tweets_set.h5', key='train_tweets_set', mode='w')\n",
        "\n",
        "\n",
        "def write_train_knesset_members(train_knesset_members_set):\n",
        "  \"\"\"\n",
        "   writes the data of the training knesset members dataset\n",
        "  \"\"\"\n",
        "  train_knesset_members_set.to_csv('/content/drive/MyDrive/model_files/data/train_knesset_members_set.csv', encoding='utf-8-sig',index=False)\n",
        "  train_knesset_members_set.to_hdf('/content/drive/MyDrive/model_files/data/train_knesset_members_set.h5', key='train_knesset_members_set', mode='w')\n",
        "\n",
        "def write_test_tweets(test_tweets_set):\n",
        "  \"\"\"\n",
        "   writes the tweets of the testing tweets dataset\n",
        "  \"\"\"\n",
        "  test_tweets_set.to_csv('/content/drive/MyDrive/model_files/data/test_tweets_set.csv', encoding='utf-8-sig',index=False)\n",
        "  test_tweets_set.to_hdf('/content/drive/MyDrive/model_files/data/test_tweets_set.h5', key='test_tweets_set', mode='w')\n",
        "\n",
        "def write_test_knesset_members(test_knesset_members_set):\n",
        "  \"\"\"\n",
        "  writes the data of the testing knesset members dataset\n",
        "  \"\"\"\n",
        "  test_knesset_members_set.to_csv('/content/drive/MyDrive/model_files/data/test_knesset_members_set.csv', encoding='utf-8-sig',index=False)\n",
        "  test_knesset_members_set.to_hdf('/content/drive/MyDrive/model_files/data/test_knesset_members_set_set.h5', key='test_knesset_members_set', mode='w')\n",
        "\n",
        "def split(knesset_member_ds, tweets_ds, train_percentage):\n",
        "  \"\"\"\n",
        "  splits by a given percentage the knesset members and then splits the tweets according to their author into train\n",
        "  and test tweets.\n",
        "  :param tweets_ds: the tweets' dataset\n",
        "  :param knesset_member_ds: the knesset members dataset\n",
        "  :param train_percentage: the parentage of the split\n",
        "  :return the split knesset members in their corresponding tweets\n",
        "  \"\"\"\n",
        "  random.seed(42)  # in order to get consistent data\n",
        "  number_of_knesset_members = knesset_member_ds['מס סידורי'].count()\n",
        "  number_of_train = int((number_of_knesset_members * (train_percentage / 100)))\n",
        "  # get the serial number of the training and test set knesset members randomly\n",
        "  train_serial = random.sample(range(1, number_of_knesset_members + 1), number_of_train)\n",
        "  test_serial = [serial for serial in range(1, number_of_knesset_members + 1) if serial not in train_serial]\n",
        "  # get the test and train the knesset members information by the serial number\n",
        "  train_knesset_members_set = knesset_member_ds.loc[knesset_member_ds['מס סידורי'].isin(train_serial)]\n",
        "  test_knesset_members_set = knesset_member_ds.loc[knesset_member_ds['מס סידורי'].isin(test_serial)]\n",
        "  # get the test and train knesset members username\n",
        "  train_knesset_members_set_usernames = train_knesset_members_set['שם משתמש']\n",
        "  test_knesset_members_set_usernames = test_knesset_members_set['שם משתמש']\n",
        "  # get the test and train tweets by the knesset members username\n",
        "  train_tweets_set = tweets_ds.loc[tweets_ds['User'].isin(train_knesset_members_set_usernames)]\n",
        "  test_tweets_set = tweets_ds.loc[tweets_ds['User'].isin(test_knesset_members_set_usernames)]\n",
        "  return train_knesset_members_set, train_tweets_set, test_knesset_members_set, test_tweets_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHj7YLkjcZ9K",
        "outputId": "5dcd6d98-a3af-45ba-bd4b-227a80ed6cdd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py:2703: PerformanceWarning: \n",
            "your performance may suffer as PyTables will pickle object types that it cannot\n",
            "map directly to c-types [inferred_type->mixed,key->block1_values] [items->Index(['User', 'Tweet'], dtype='object')]\n",
            "\n",
            "  pytables.to_hdf(\n"
          ]
        }
      ],
      "source": [
        "tweets_ds = read_tweets()\n",
        "knesset_member_ds = read_knesset_members()\n",
        "train_knesset_members_set, train_tweets_set, test_knesset_members_set, test_tweets_set = split(knesset_member_ds,\n",
        "                                                                                                  tweets_ds,\n",
        "                                                                                                  TRAIN_PERCENTAGE)\n",
        "write_train_tweets(train_tweets_set)\n",
        "write_train_knesset_members(train_knesset_members_set)\n",
        "write_test_tweets(test_tweets_set)\n",
        "write_test_knesset_members(test_knesset_members_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Amq3PASuFHRQ"
      },
      "source": [
        "###TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1LIUz3xFQd2"
      },
      "outputs": [],
      "source": [
        "NT = 3  # the minimum nuber of tweets a word in tf idf should show\n",
        "SPECIAL_CHARCATERS_REMOVAL=r\"(?u)\\b\\w+[\\\"\\'״]*\\w\\b\" # tokenize by \" \" and \"/n\" and allow abbreviations\n",
        "TOKEN_PATTERN=\"[^ \\n]+\" # baseline tokenize, only by space and end of row\n",
        "\n",
        "# Gets all the stop words from the file\n",
        "pattern = re.compile(SPECIAL_CHARCATERS_REMOVAL)\n",
        "with open(\"/content/drive/MyDrive/model_files/data/stop_words.txt\", 'r') as f:\n",
        "        STOP_WORDS = pattern.findall(f.read())\n",
        "\n",
        "# Insert all the abbreviations into a dictionary\n",
        "ABBREVIATIONS = {}\n",
        "with open(\"/content/drive/MyDrive/model_files/data/abbreviations.txt\") as f:\n",
        "    for line in f:\n",
        "        key, value = line.split(\"-\")\n",
        "        ABBREVIATIONS[key.strip()] = value.strip()\n",
        "\n",
        "# To preprocess open abbrevaitons\n",
        "def expand_abbreviations(text):\n",
        "    \"\"\"\n",
        "    a function that gets a text and replace all abbreviations with thier expnaded form\n",
        "    \"\"\"\n",
        "    for match in re.finditer(SPECIAL_CHARCATERS_REMOVAL, text):\n",
        "        abbreviation = match.group()\n",
        "        normelized_abbreviation=abbreviation\n",
        "        # normalize all \" \n",
        "        if \"״\" in abbreviation:\n",
        "          normelized_abbreviation=abbreviation.replace(\"״\",\"\\\"\")\n",
        "        if \"''\" in abbreviation:\n",
        "          normelized_abbreviation=abbreviation.replace(\"''\",\"\\\"\")\n",
        "        # if its in the, replace with expanded form\n",
        "        if normelized_abbreviation in ABBREVIATIONS:\n",
        "            text = text.replace(abbreviation, ABBREVIATIONS[normelized_abbreviation])\n",
        "    return text\n",
        "\n",
        "VECTORIZERS={ # the tf-idf vectorizers by preprocessing method\n",
        "    'N' : TfidfVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN),#None, baseline\n",
        "    # only one preprocessing method\n",
        "    'S':TfidfVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS),#Stop word removal\n",
        "    'C':TfidfVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL),#special Characters removal\n",
        "    'O':TfidfVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN, preprocessor=expand_abbreviations),#Open abbreviations\n",
        "    # pairing preprocssing methods\n",
        "    #'SC':TfidfVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS),\n",
        "    #'SO':TfidfVectorizer(min_df=NT,lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "    #'CO':TfidfVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,preprocessor=expand_abbreviations),\n",
        "    # all preprocessing methods\n",
        "    #'SCO':TfidfVectorizer(min_df=NT,lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEjE6EfnEebY"
      },
      "outputs": [],
      "source": [
        "def read_train_tweets():\n",
        "  \"\"\"\n",
        "  reads the data from the train tweets sheet\n",
        "  :return: the data as pandas data set\n",
        "  \"\"\"\n",
        "  \n",
        "  store = pd.HDFStore('/content/drive/MyDrive/model_files/data/train_tweets_set.h5')\n",
        "  df = store.select('train_tweets_set')\n",
        "  store.close()\n",
        "\n",
        "  columns_to_include = [\"Tweet\"]\n",
        "  df = df[columns_to_include]\n",
        "  df.dropna(subset=[\"Tweet\"], inplace=True)\n",
        "  return df\n",
        "\n",
        "def get_words(tweets,preprocessing):\n",
        "  \"\"\"\n",
        "  returns all the tweets words by the preprocessing method\n",
        "  \"\"\"\n",
        "  corpus=tweets\n",
        "  if \"O\" in preprocessing:\n",
        "    corpus=corpus.apply(expand_abbreviations)\n",
        "  \n",
        "  if \"C\" in preprocessing:\n",
        "    pattern = re.compile(SPECIAL_CHARCATERS_REMOVAL)\n",
        "  else:\n",
        "    pattern = re.compile(TOKEN_PATTERN)\n",
        "\n",
        "  corpus_words=[]\n",
        "  for document in tqdm(corpus):\n",
        "      corpus_words+= pattern.findall(document)\n",
        "  return corpus_words\n",
        "\n",
        "def get_terms():\n",
        "  columns= [\"nt\", \"f\", \"tf\",\"idf\",\"tfidf\"]\n",
        "  meanings= [\"Number of different documents that the word appears in.\",\n",
        "   \"Number of appearances of the word in all documents.\",\n",
        "   \"Term frequency.\",\n",
        "   \"Inverse document frequency.\",\n",
        "   \"Term fruquency multiplied by inverse document frequency.\"]\n",
        "  return columns, meanings\n",
        "   \n",
        "  \n",
        "def write_tf_idf_chart(preprocessing, vectorizer):\n",
        "  \"\"\"\n",
        "  writes the tf idf chart by the given preprocessing vectorizer\n",
        "  \"\"\"\n",
        "  train_tweets = read_train_tweets()\n",
        "  sparse_matrix = vectorizer.fit_transform(train_tweets[\"Tweet\"])\n",
        "  dense_matrix = sparse_matrix.toarray()\n",
        "\n",
        "  # Get the vocabulary from the vectorizer object\n",
        "  vocab = vectorizer.get_feature_names_out()\n",
        "  df = pd.DataFrame(columns=['word', 'nt', 'f', 'tf', 'idf', 'tf-idf',' ','Column','Meaning'])\n",
        "\n",
        "  # Gather all the tweets words to calculate words frequency\n",
        "  print(\"Getting words from tweets:\")\n",
        "  tweets_words=get_words(train_tweets[\"Tweet\"],preprocessing)\n",
        "  print(\"done!\")\n",
        "\n",
        "  print(\"calculating 'nt' and 'f':\")\n",
        "  for word in tqdm(vocab):\n",
        "      # The column of tf-idf values of the specific word, as the rows are the tweets \n",
        "      word_column=dense_matrix[:, vectorizer.vocabulary_[word]]\n",
        "      # Count the number of tweets the word appears in\n",
        "      nt = np.count_nonzero(word_column)\n",
        "      # Count the number of times the word is used in all tweets\n",
        "      f = tweets_words.count(word)\n",
        "      # append all the data we collected so far to each word to the data frame\n",
        "      df = df.append({'word': word, 'nt': nt, 'f': f, 'tf': 0, 'idf': 0, 'tf-idf': 0}, ignore_index=True)\n",
        "  print(\"done!\")\n",
        "\n",
        "  # the numebr of words in total is the sum of the f of each word\n",
        "  sig_f=df['f'].sum()\n",
        "\n",
        "  print(\"calculating 'tf','idf' and 'tf-idf':\")\n",
        "  for word in tqdm(vocab):\n",
        "      # the index of the word\n",
        "      index = df.index[df['word'] == word][0]\n",
        "      f = df.at[index, 'f']\n",
        "      nt = df.at[index, 'nt']\n",
        "      # Calculate the tf value\n",
        "      tf = f / sig_f\n",
        "      # shape[0] is the numebr of rows, aka the numebr of tweets\n",
        "      n = dense_matrix.shape[0]\n",
        "      # Calculate the idf value\n",
        "      idf = np.log(n / (nt + 1))\n",
        "      # Calculate the tf-idf value\n",
        "      tf_idf = tf * idf\n",
        "      # update the word's ramaining information to the dataframe\n",
        "      df.at[index, 'tf'] = tf\n",
        "      df.at[index, 'idf'] = idf\n",
        "      df.at[index, 'tf-idf'] = tf_idf\n",
        "  print(\"done!\")\n",
        "\n",
        "  #sort in descending order\n",
        "  df = df.sort_values('tf-idf', ascending=False)\n",
        "  #add vocabulary\n",
        "  columns, meanings = get_terms()\n",
        "  for i in range(len(columns)):\n",
        "    df.at[df.index[i], \"Column\"] = columns[i]\n",
        "    df.at[df.index[i], \"Meaning\"] = meanings[i]\n",
        "  # write to csv\n",
        "  df.to_csv(f'/content/drive/MyDrive/model_files/tf idf charts/{preprocessing}_words_tfidf_chart.csv', encoding='utf-8-sig',index=False)\n",
        "  # write to h5\n",
        "  df.to_hdf(f'/content/drive/MyDrive/model_files/tf idf charts/{preprocessing}_words_tfidf_chart.h5', key='words_df', mode='w')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qQt-PTF05AT",
        "outputId": "caa76798-85fe-4cc1-826d-b3d0ac7e786c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------\n",
            "writing the N tf-idf chart:\n",
            "Getting words from tweets:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:00<00:00, 73209.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "calculating 'nt' and 'f':\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19984/19984 [20:40<00:00, 16.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "calculating 'tf','idf' and 'tf-idf':\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19984/19984 [00:47<00:00, 420.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py:2703: PerformanceWarning: \n",
            "your performance may suffer as PyTables will pickle object types that it cannot\n",
            "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['word', 'nt', 'f', 'tf', 'idf', 'tf-idf', ' ', 'Column', 'Meaning'], dtype='object')]\n",
            "\n",
            "  pytables.to_hdf(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------\n",
            "------------------------------------------\n",
            "writing the S tf-idf chart:\n",
            "Getting words from tweets:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:00<00:00, 78569.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "calculating 'nt' and 'f':\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19571/19571 [19:58<00:00, 16.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "calculating 'tf','idf' and 'tf-idf':\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19571/19571 [00:53<00:00, 362.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "writing the C tf-idf chart:\n",
            "Getting words from tweets:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:00<00:00, 52077.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "calculating 'nt' and 'f':\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 18417/18417 [17:58<00:00, 17.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "calculating 'tf','idf' and 'tf-idf':\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 18417/18417 [00:42<00:00, 433.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "writing the O tf-idf chart:\n",
            "Getting words from tweets:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:00<00:00, 30998.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "calculating 'nt' and 'f':\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19771/19771 [20:23<00:00, 16.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "calculating 'tf','idf' and 'tf-idf':\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19771/19771 [00:46<00:00, 427.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#itirate over each preprocessing method and writes the tfidf chart\n",
        "for preprocessing, vectorizer in VECTORIZERS.items():\n",
        "  print(\"------------------------------------------\")\n",
        "  print(f\"writing the {preprocessing} tf-idf chart:\")\n",
        "  write_tf_idf_chart(preprocessing, vectorizer)\n",
        "  print(\"------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u29BNWrmfkc",
        "outputId": "75a26470-545c-4a1e-9fda-2f206b87f2e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:00<00:00, 79120.35it/s]\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# writes the abbreviations file\n",
        "with open(\"/content/drive/MyDrive/model_files/data/abbreviations_not_expanded_.txt\", \"w\") as f:\n",
        "    train_tweets = read_train_tweets()\n",
        "    words=get_words(train_tweets[\"Tweet\"],\"special characters removal\")\n",
        "    abbreviations=[]\n",
        "    print(words)\n",
        "    for word in words:\n",
        "        # Check if the word matches the regex and includes the character \"?\n",
        "        if ((\"\\\"\" in word)or (\"\\'\" in word) or (\"״\" in word)) and not( word in abbreviations ):\n",
        "            # Write the word to the text file\n",
        "            abbreviations.append(word)\n",
        "            f.write(word + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF0jF1GlJovI"
      },
      "source": [
        "###Vectorize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKxeKm-dKDAb"
      },
      "outputs": [],
      "source": [
        "NUMBER_OF_WORDS = [5000,4000,3000,2000,1000]  # the number of the highest tfidf words\n",
        "CLASSIFY = [\n",
        "\"קבוצת גיל\"\n",
        ",\"מין\"\n",
        ",\"0-ימין\"\n",
        ",\"1-ימין מרכז\"\n",
        ",\"2-שמאל מרכז\"\n",
        ",\"3-שמאל\"\n",
        ",\"0-ליכוד\"\n",
        ",\"1-יש עתיד\"\n",
        ",\"2-הציונות הדתית\"\n",
        ",\"3-מרצ\"\n",
        ",\"4-תקווה חדשה\"\n",
        ",\"5-ימינה\"\n",
        ",\"6- ישראל ביתנו\"\n",
        ",\"7-כחול לבן\"\n",
        ",\"8-ש\\\"ס\"\n",
        ",\"9-הרשימה המשותפת\"\n",
        ",\"10-העבודה\"\n",
        ",\"11-רע\\\"מ\"\n",
        ",\"12-ח\\\"כ בודד\"\n",
        ",\"0-ספרדי\"\n",
        ",\"1-אשכנזי\"\n",
        ",\"דתי אורתודוקסי\"\n",
        ",\"לא חרדי-0\"\n",
        ",\"חרדי-1\"\n",
        ",\"יהודי\"]  # the classification attribute of the tweet vectors\n",
        "SPECIAL_CHARCATERS_REMOVAL=r\"(?u)\\b\\w+[\\\"\\'״]*\\w\\b\" # tokenize by \" \" and \"/n\" and allow abbreviations\n",
        "TOKEN_PATTERN=\"[^ \\n]+\" # baseline tokenize, only by space and end of row\n",
        "\n",
        "# Gets all the stop words from the file\n",
        "with open(\"/content/drive/MyDrive/model_files/data/stop_words.txt\", 'r') as f:\n",
        "        STOP_WORDS = f.read().splitlines()\n",
        "\n",
        "# Insert all the abbreviations into a dictionary\n",
        "ABBREVIATIONS = {}\n",
        "with open(\"/content/drive/MyDrive/model_files/data/abbreviations.txt\") as f:\n",
        "    for line in f:\n",
        "        key, value = line.split(\"-\")\n",
        "        ABBREVIATIONS[key.strip()] = value.strip()\n",
        "\n",
        "# To preprocess open abbrevaitons\n",
        "def expand_abbreviations(text):\n",
        "    \"\"\"\n",
        "    a function that gets a text and replace all abbreviations with thier expnaded form\n",
        "    \"\"\"\n",
        "    for match in re.finditer(SPECIAL_CHARCATERS_REMOVAL, text):\n",
        "        abbreviation = match.group()\n",
        "        normelized_abbreviation=abbreviation\n",
        "        # normalize all \" \n",
        "        if \"״\" in abbreviation:\n",
        "          normelized_abbreviation=abbreviation.replace(\"״\",\"\\\"\")\n",
        "        if \"''\" in abbreviation:\n",
        "          normelized_abbreviation=abbreviation.replace(\"''\",\"\\\"\")\n",
        "        # if its in the, replace with expanded form\n",
        "        if normelized_abbreviation in ABBREVIATIONS:\n",
        "            text = text.replace(abbreviation, ABBREVIATIONS[normelized_abbreviation])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkBlWyYPJtPr"
      },
      "outputs": [],
      "source": [
        "def read_train_tweets():\n",
        "  \"\"\"\n",
        "  reads the data from the train tweets sheet\n",
        "  :return: the data as pandas data set\n",
        "  \"\"\"\n",
        "  store = pd.HDFStore('/content/drive/MyDrive/model_files/data/train_tweets_set.h5')\n",
        "  df = store.select('train_tweets_set')\n",
        "  store.close()\n",
        "\n",
        "  columns_to_include = [\"User\",\"Tweet\"]\n",
        "  df = df[columns_to_include]\n",
        "  df.dropna(subset=[\"Tweet\"], inplace=True)\n",
        "  return df\n",
        "\n",
        "def read_test_tweets():\n",
        "  \"\"\"\n",
        "  reads the data from the test tweets sheet\n",
        "  :return: the data as pandas data set\n",
        "  \"\"\"\n",
        "  store = pd.HDFStore('/content/drive/MyDrive/model_files/data/test_tweets_set.h5')\n",
        "  df = store.select('test_tweets_set')\n",
        "  store.close()\n",
        "\n",
        "  columns_to_include = [\"User\",\"Tweet\"]\n",
        "  df = df[columns_to_include]\n",
        "  df.dropna(subset=[\"Tweet\"], inplace=True)\n",
        "  return df\n",
        "\n",
        "def read_train_knesset_members():\n",
        "  \"\"\"\n",
        "  reads the data from the train knesst members sheet\n",
        "  :return: the data as pandas data set\n",
        "  \"\"\"\n",
        "  train_knesset_members= pd.read_hdf('/content/drive/MyDrive/model_files/data/train_knesset_members_set.h5',key='train_knesset_members_set',)\n",
        "  return train_knesset_members\n",
        "\n",
        "def read_test_knesset_members():\n",
        "  \"\"\"\n",
        "  reads the data from the test knesst members sheet\n",
        "  :return: the data as pandas data set\n",
        "  \"\"\"\n",
        "  test_knesset_members= pd.read_hdf('/content/drive/MyDrive/model_files/data/test_knesset_members_set_set.h5',key='test_knesset_members_set',)\n",
        "  return test_knesset_members\n",
        "\n",
        "def write_classification(tweets, knesset, classification,set):\n",
        "  \"\"\"\n",
        "  return the tweet vector with the classification\n",
        "  :param tweets: the tweets set\n",
        "  :param knesset_members_train_set: the knesset members information\n",
        "  :param classification: the desired classification\n",
        "  :return: the classified tweets dataframe\n",
        "  \"\"\"\n",
        "  classified_tweets_df = pd.DataFrame(columns=[\"class: \"+classification])\n",
        "  print(f\"writing {set} tweets classification:\")\n",
        "  for index in tqdm(range(len(tweets.index))):\n",
        "      username = tweets.iat[index,0]\n",
        "      tweet = tweets.iat[index, 1]\n",
        "      if not pd.isna(tweet) and not pd.isna(username):\n",
        "          classification_index = knesset.index[knesset['שם משתמש'] == username][0]\n",
        "          tweet_classification = knesset.at[classification_index, classification]\n",
        "          tweet_classification = str(tweet_classification)\n",
        "          if cls==\"קבוצת גיל\":\n",
        "            if tweet_classification == \"עד 50\":\n",
        "              tweet_classification = 0\n",
        "            else:\n",
        "              tweet_classification = 1\n",
        "          else:\n",
        "              tweet_classification = re.findall(r'\\d+',tweet_classification)[0]\n",
        "          classified_tweets_df.loc[len(classified_tweets_df.index)] = [tweet_classification]\n",
        "  print(\"done!\")\n",
        "  classified_tweets_df.to_csv(f'/content/drive/MyDrive/model_files/classification/{set}_tweets_classification_{classification}.csv', encoding='utf-8-sig', index=False)\n",
        "  classified_tweets_df.to_hdf(f'/content/drive/MyDrive/model_files/classification/{set}_tweets_classification_{classification}.h5', key='classification', mode='w')\n",
        "\n",
        "def write_vectors(tfidf_vocabulary,vectors,set,amount,preprocessing):\n",
        "  \"\"\"\n",
        "  write the given set tfidf vectors by the tf idf vocabulary\n",
        "  \"\"\"\n",
        "  dense_vectors = vectors.toarray()\n",
        "  df = pd.DataFrame(dense_vectors, columns=tfidf_vocabulary)\n",
        "  df.to_csv(f'/content/drive/MyDrive/model_files/vectors/{preprocessing}_{set}_tweets_vectors_{amount}.csv', encoding='utf-8-sig',index=False)\n",
        "  df.to_hdf(f'/content/drive/MyDrive/model_files/vectors/{preprocessing}_{set}_tweets_vectors_{amount}.h5', key='vectors', mode='w')\n",
        "\n",
        "def get_words(amount,preprocessing):\n",
        "  \"\"\"\n",
        "  get the highest tf-idf words by a given amount from the df-idf chart\n",
        "  :param amount: the amount of words\n",
        "  :return: the given amount of words with the highest TF-IDF value\n",
        "  \"\"\"\n",
        "  words = pd.read_hdf(f'/content/drive/MyDrive/model_files/tf idf charts/{preprocessing}_words_tfidf_chart.h5',key=\"words_df\")\n",
        "  words = words.head(amount)\n",
        "  words = words['word'].tolist()\n",
        "  return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AxQGVW0Gc3WY",
        "outputId": "2795bff7-6db7-4cb2-e2c1-b8523969f648"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 1/4 [01:33<04:39, 93.07s/it]/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['יד', 'ידי'] not in stop_words.\n",
            "  warnings.warn(\n",
            " 75%|███████▌  | 3/4 [04:54<01:39, 99.15s/it]/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:1322: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n",
            "100%|██████████| 4/4 [06:30<00:00, 97.69s/it]\n",
            "100%|██████████| 4/4 [05:38<00:00, 84.57s/it]\n",
            "100%|██████████| 4/4 [04:33<00:00, 68.45s/it]\n",
            "100%|██████████| 4/4 [03:23<00:00, 50.85s/it]\n",
            "100%|██████████| 4/4 [02:38<00:00, 39.52s/it]\n"
          ]
        }
      ],
      "source": [
        "train_tweets = read_train_tweets()\n",
        "train_knesset_members=read_train_knesset_members()\n",
        "test_tweets = read_test_tweets()\n",
        "test_knesset_members=read_test_knesset_members()\n",
        "for amount in NUMBER_OF_WORDS:\n",
        "  VECTORIZERS={ # the tf-idf vectorizers by preprocessing method\n",
        "    'N' : TfidfVectorizer(vocabulary=get_words(amount,'N'),lowercase=False, token_pattern=TOKEN_PATTERN),#None, baseline\n",
        "    # only one preprocessing method\n",
        "    'S':TfidfVectorizer(vocabulary=get_words(amount,'S'),lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS),#Stop word removal\n",
        "    'C':TfidfVectorizer(vocabulary=get_words(amount,'C'),lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL),#special Characters removal\n",
        "    'O':TfidfVectorizer(vocabulary=get_words(amount,'O'),lowercase=False, token_pattern=TOKEN_PATTERN, preprocessor=expand_abbreviations),#Open abbreviations\n",
        "    # pairing preprocssing methods\n",
        "    #'SC':TfidfVectorizer(vocabulary=get_words(amount,'SC'),lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS),\n",
        "    #'SO':TfidfVectorizer(vocabulary=get_words(amount,'SO'),lowercase=False, token_pattern=TOKEN_PATTERN,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "    #'CO':TfidfVectorizer(vocabulary=get_words(amount,'CO'),lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,preprocessor=expand_abbreviations),\n",
        "    # all preprocessing methods\n",
        "    #'SCO':TfidfVectorizer(vocabulary=get_words(amount,'SCO'),lowercase=False, token_pattern=SPECIAL_CHARCATERS_REMOVAL,stop_words=STOP_WORDS,preprocessor=expand_abbreviations),\n",
        "  }\n",
        "  for preprocessing, vectorizer in tqdm(VECTORIZERS.items()):\n",
        "    tfidf_vocabulary=get_words(amount,preprocessing)\n",
        "    train_vectors = vectorizer.fit_transform(train_tweets[\"Tweet\"])\n",
        "    write_vectors(tfidf_vocabulary,train_vectors,\"train\",amount,preprocessing)\n",
        "\n",
        "    test_vectors = vectorizer.fit_transform(test_tweets[\"Tweet\"])\n",
        "    write_vectors(tfidf_vocabulary,test_vectors,\"test\",amount,preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLglu-bUVFct",
        "outputId": "3d7de821-20ed-4a0d-8429-12be15882f8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "writing קבוצת גיל\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:26<00:00, 490.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py:2703: PerformanceWarning: \n",
            "your performance may suffer as PyTables will pickle object types that it cannot\n",
            "map directly to c-types [inferred_type->integer,key->block0_values] [items->Index(['class: קבוצת גיל'], dtype='object')]\n",
            "\n",
            "  pytables.to_hdf(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 528.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing מין\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:26<00:00, 485.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 509.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 0-ימין\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:26<00:00, 479.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 515.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 1-ימין מרכז\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:27<00:00, 467.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 513.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 2-שמאל מרכז\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:26<00:00, 482.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 510.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 3-שמאל\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:27<00:00, 467.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 506.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 0-ליכוד\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:26<00:00, 486.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 511.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 1-יש עתיד\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:27<00:00, 466.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 514.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 2-הציונות הדתית\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:27<00:00, 474.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 488.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 3-מרצ\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:27<00:00, 469.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 499.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 4-תקווה חדשה\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:27<00:00, 468.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 497.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 5-ימינה\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:27<00:00, 462.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 517.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 6- ישראל ביתנו\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:26<00:00, 475.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 496.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 7-כחול לבן\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:26<00:00, 483.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 509.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 8-ש\"ס\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:26<00:00, 476.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 488.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 9-הרשימה המשותפת\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:28<00:00, 452.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 521.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 10-העבודה\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:26<00:00, 478.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 507.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 11-רע\"מ\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:27<00:00, 474.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 519.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 12-ח\"כ בודד\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:26<00:00, 479.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 515.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 0-ספרדי\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:27<00:00, 463.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 511.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing 1-אשכנזי\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:27<00:00, 473.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 512.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing דתי אורתודוקסי\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:26<00:00, 480.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 517.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing לא חרדי-0\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:26<00:00, 478.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 516.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing חרדי-1\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:27<00:00, 463.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 503.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing יהודי\n",
            "writing train tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12825/12825 [00:26<00:00, 482.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n",
            "writing test tweets classification:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4330/4330 [00:08<00:00, 510.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done!\n"
          ]
        }
      ],
      "source": [
        "train_tweets = read_train_tweets()\n",
        "train_knesset_members=read_train_knesset_members()\n",
        "test_tweets = read_test_tweets()\n",
        "test_knesset_members=read_test_knesset_members()\n",
        "for cls in CLASSIFY:\n",
        "  print(\"writing \"+cls)\n",
        "  write_classification(train_tweets,train_knesset_members,cls, \"train\")\n",
        "  write_classification(test_tweets,test_knesset_members,cls, \"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4GQ5f58FpNH"
      },
      "source": [
        "##ML Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMZfzbtThZaG"
      },
      "source": [
        "###Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo7eUYDpFwcR"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 1 # in order to get consistent results for the random forest algorithem\n",
        "CLASSIFY = [\n",
        "#\"קבוצת גיל\"//all\n",
        "#,\"מין\"//all\n",
        "#,\"0-ימין\",//all\n",
        "#\"1-ימין מרכז\"\n",
        "#,\"2-שמאל מרכז\"//all\n",
        "#,\"3-שמאל\"\n",
        "#,\"0-ליכוד\"//all\n",
        "#,\"1-יש עתיד\"\n",
        "#,\"2-הציונות הדתית\"//all\n",
        "#,\"3-מרצ\"//all\n",
        "#,\"4-תקווה חדשה\"//all\n",
        "#,\"5-ימינה\"//all\n",
        "#,\"6- ישראל ביתנו\"//all\n",
        "#,\"7-כחול לבן\"//all\n",
        "#,\"8-ש\\\"ס\"//all\n",
        "#,\"9-הרשימה המשותפת\"//all\n",
        "#,\"10-העבודה\"//all\n",
        "#,\"11-רע\\\"מ\"\n",
        "#,\n",
        "\"12-ח\\\"כ בודד\"\n",
        "#,\"0-ספרדי\"//all\n",
        "#,\"1-אשכנזי\"//all\n",
        "#,\"דתי אורתודוקסי\"//all\n",
        "#,\"לא חרדי-0\"//all\n",
        "#,\"חרדי-1\"//all\n",
        "#,\"יהודי\"//all\n",
        "]  # the classification attribute of the tweet vectors\n",
        "NUM_OF_WORDS=[1000,2000,3000,4000,5000]\n",
        "ML_CLASSIFIERS= [ # the classifers we use\n",
        "\"RF\"\n",
        "#,\"SVM\"\n",
        ",\"MLP\"\n",
        ",\"MNB\"\n",
        ",\"LR\"\n",
        "]\n",
        "OVERSAMPLERS=[#\"none\",\n",
        "              \"random\",\n",
        "              #\"smote\",\n",
        "              #\"adasyn\"\n",
        "              ]\n",
        "# the preprocessing methods\n",
        "PREPROCESSORS=[#'N','S','C',\n",
        "               'O'\n",
        "               ]#,'SC','SO','CO','SCO']\n",
        "RANDOM_STATE=42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeU-atbnG1yO"
      },
      "outputs": [],
      "source": [
        "def get_sets_vectors(class_name,amount,preprocessing,oversampler):\n",
        "  \"\"\"\n",
        "  reads and returns the vectors and classifcation of the train and test sets.\n",
        "  \"\"\"\n",
        "  x_train=pd.read_hdf(f'/content/drive/MyDrive/model_files/vectors/{preprocessing}_train_tweets_vectors_{amount}.h5',key='vectors')\n",
        "  y_train=pd.read_hdf(f'/content/drive/MyDrive/model_files/classification/train_tweets_classification_{class_name}.h5',key='classification').iloc[:,0] \n",
        "  y_train=np.array(y_train).astype(int)                          \n",
        "  x_test=pd.read_hdf(f'/content/drive/MyDrive/model_files/vectors/{preprocessing}_test_tweets_vectors_{amount}.h5',key='vectors')\n",
        "  y_test=pd.read_hdf(f'/content/drive/MyDrive/model_files/classification/test_tweets_classification_{class_name}.h5',key='classification').iloc[:,0]\n",
        "  y_test=np.array(y_test).astype(int)\n",
        "\n",
        "  if oversampler != \"none\":\n",
        "    if oversampler == \"random\":\n",
        "      oversampler = RandomOverSampler(random_state=RANDOM_STATE)\n",
        "    if oversampler == \"smote\":\n",
        "      oversampler = SMOTE(random_state=RANDOM_STATE)\n",
        "    if oversampler == \"adasyn\":\n",
        "      oversampler = ADASYN(random_state=RANDOM_STATE)\n",
        "\n",
        "    x_train, y_train = oversampler.fit_resample(x_train, y_train)      \n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "\n",
        "def score_classifier(y_test, predictions, classifier,f1_classifiers_scores):\n",
        "  \"\"\"\n",
        "  returns the given classifers score by comparing the predictions and true classifies.\n",
        "  \"\"\"\n",
        "  # calculate the classifier's score\n",
        "  f1 = f1_score(y_test, predictions)\n",
        "  #precision = precision_score(y_test, predictions, zero_division=0)\n",
        "  #accuracy = accuracy_score(y_test, predictions)\n",
        "  #recall = recall_score(y_test, predictions)\n",
        "  #roc_auc = roc_auc_score(y_test, predictions)\n",
        "\n",
        "  # add the classifer score to the dictionary\n",
        "  f1_classifiers_scores[classifier] = f1\n",
        "  #precision_classifiers_scores[classifier] = precision\n",
        "  #accuracy_classifiers_scores[classifier] = accuracy\n",
        "  #recall_classifiers_scores[classifier] = recall\n",
        "  #auc_classifiers_scores[classifier] = roc_auc\n",
        "\n",
        "  #return f1, precision, accuracy, recall, roc_auc\n",
        "  return f1,0,0,0,0\n",
        "\n",
        "def print_classifier_score(f1, precision, accuracy, recall, roc_auc, classifier):\n",
        "  \"\"\"\n",
        "  prints the scores of the given classifier.\n",
        "  \"\"\"\n",
        "  print(\"---------------------\")\n",
        "  print(classifier + \" score:\")\n",
        "  print(\"F1: \" + str(f1))\n",
        "  print(\"Precision: \" + str(precision))\n",
        "  print(\"Accuracy: \" + str(accuracy))\n",
        "  print(\"Recall: \" + str(recall))\n",
        "  print(\"ROC Under Curve: \" + str(roc_auc))\n",
        "  print(\"---------------------\")\n",
        "\n",
        "def run_classifier(x_train, x_test, y_train, y_test,classifier,f1_classifiers_scores):\n",
        "  \"\"\"\n",
        "  runs the specified classifier with the given train and test sets.\n",
        "  \"\"\"\n",
        "  # create a scikit classifier by the given classifier name\n",
        "  if classifier==\"RF\":\n",
        "      clf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "  elif classifier==\"SVM\":\n",
        "      clf = SVC()\n",
        "  elif classifier==\"MLP\":\n",
        "      clf = MLPClassifier()\n",
        "  elif classifier==\"MNB\":\n",
        "      clf = MultinomialNB()\n",
        "  elif classifier==\"LR\":\n",
        "      clf = LogisticRegression()\n",
        "  else:\n",
        "      raise ValueError(\"unknown classifier given\")\n",
        "  clf.fit(x_train, y_train)\n",
        "  predictions = clf.predict(x_test)\n",
        "\n",
        "  f1, precision, accuracy, recall, roc_auc = score_classifier(y_test, predictions, classifier,f1_classifiers_scores)\n",
        "  #print_classifier_score(f1, precision, accuracy, recall, roc_auc, classifier)\n",
        "\n",
        "\n",
        "def ml_classifiers(x_train, x_test, y_train, y_test,classifiers):\n",
        "  \"\"\"\n",
        "  creates the classifers and prints their scores. \n",
        "  \"\"\"\n",
        "  # scores dictionaries to store and summary by measure\n",
        "  f1_classifiers_scores = {}\n",
        "  #accuracy_classifiers_scores = {}\n",
        "  for classifier in tqdm(classifiers):\n",
        "    run_classifier(x_train, x_test, y_train, y_test,classifier,f1_classifiers_scores)\n",
        "\n",
        "  return f1_classifiers_scores\n",
        "\n",
        "def create_score_chart(cls,oversampling,preprocessing):\n",
        "  df_columns = []\n",
        "  # Create a data frame with the title row\n",
        "  df_columns.append(\"Number of words\")\n",
        "  df_columns = df_columns + ML_CLASSIFIERS\n",
        "  df = pd.DataFrame(columns=df_columns)\n",
        "    \n",
        "  # Create an array of empty strings\n",
        "  empty_strings = np.empty(len(df_columns), dtype='U64')\n",
        "  empty_strings[:] = ''\n",
        "\n",
        "  # Set the first element of the array to a specific value\n",
        "  empty_strings[0] = \"f1 results of \"+oversampling+ \" oversampling after preprocess of \" + cls + \" by \" + preprocessing\n",
        "  title = pd.DataFrame(data=[empty_strings], columns=df.columns)\n",
        "  for amount in NUM_OF_WORDS:\n",
        "      print(f\"running ml methods for {amount} of words classifying {cls} with preprocessing '{preprocessing}'...\")\n",
        "      x_train, x_test, y_train, y_test=get_sets_vectors(cls,amount,preprocessing,oversampling)\n",
        "      f1_classifiers_scores = ml_classifiers(x_train, x_test, y_train, y_test,ML_CLASSIFIERS)\n",
        "      df_row = []\n",
        "      df_row.append(amount)\n",
        "      df_row = df_row + list(f1_classifiers_scores.values())\n",
        "      df.loc[len(df.index)] = df_row\n",
        "  print(tabulate(df, headers='keys', tablefmt='psql'))\n",
        "  # Concatenate the two data frames\n",
        "  print(cls+\" \"+oversampling+\" \"+preprocessing+\":\")\n",
        "  header = pd.DataFrame(data=[df_columns], columns=df_columns)\n",
        "  csv_df = pd.concat([title,header,df], ignore_index=True)\n",
        "  csv_df.to_csv(f'/content/drive/MyDrive/model_files/results/{oversampling}/csv/{cls}_by_{preprocessing}_f1_results.csv', header=False, encoding='utf-8-sig', index=False)\n",
        "  df.to_hdf(f'/content/drive/MyDrive/model_files/results/{oversampling}/h5/{cls}_by_{preprocessing}_f1_results.h5', key='results', mode='w')\n",
        "  print(\"finished wreiting \"+cls+\" \"+oversampling+\" \"+preprocessing+\":\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gE135oydI5V",
        "outputId": "b8aa63f6-ffd4-46fe-c3c7-d9be10eeeb57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running ml methods for 1000 of words classifying 12-ח\"כ בודד with preprocessing 'O'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [01:08<00:00, 17.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running ml methods for 2000 of words classifying 12-ח\"כ בודד with preprocessing 'O'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [01:50<00:00, 27.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running ml methods for 3000 of words classifying 12-ח\"כ בודד with preprocessing 'O'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [02:11<00:00, 32.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running ml methods for 4000 of words classifying 12-ח\"כ בודד with preprocessing 'O'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [02:45<00:00, 41.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running ml methods for 5000 of words classifying 12-ח\"כ בודד with preprocessing 'O'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [03:16<00:00, 49.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------------------+------+-------+-------+------+\n",
            "|    |   Number of words |   RF |   MLP |   MNB |   LR |\n",
            "|----+-------------------+------+-------+-------+------|\n",
            "|  0 |              1000 |    0 |     0 |     0 |    0 |\n",
            "|  1 |              2000 |    0 |     0 |     0 |    0 |\n",
            "|  2 |              3000 |    0 |     0 |     0 |    0 |\n",
            "|  3 |              4000 |    0 |     0 |     0 |    0 |\n",
            "|  4 |              5000 |    0 |     0 |     0 |    0 |\n",
            "+----+-------------------+------+-------+-------+------+\n",
            "12-ח\"כ בודד random O:\n",
            "finished wreiting 12-ח\"כ בודד random O:\n",
            "finished oversampling random\n"
          ]
        }
      ],
      "source": [
        "tf.test.gpu_device_name()\n",
        "for cls in CLASSIFY:\n",
        " for oversampling in OVERSAMPLERS:\n",
        "  processes = [] #open a process for each preprocessing method\n",
        "  for preprocessing in PREPROCESSORS:\n",
        "    p = Process(target=create_score_chart, args=(cls,oversampling,preprocessing))\n",
        "    p.start()\n",
        "    processes.append(p)\n",
        "\n",
        "    # wait for all processes to finish\n",
        "  for p in processes:\n",
        "    p.join()\n",
        "  print(\"finished oversampling \"+oversampling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uZ7SFIfhSp9"
      },
      "source": [
        "###One hot encoded summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5hEUpGpJjVI"
      },
      "outputs": [],
      "source": [
        "ONE_HOT_ENCODED={#all classification that are one hot encoded should be macro averged\n",
        "    \"דעה\":\n",
        "          [\"0-ימין\"\n",
        "          ,\"1-ימין מרכז\"\n",
        "          ,\"2-שמאל מרכז\"\n",
        "          ,\"3-שמאל\"],\n",
        "    \"מפלגה\":\n",
        "          [\"0-ליכוד\"\n",
        "          ,\"1-יש עתיד\"\n",
        "          ,\"2-הציונות הדתית\"\n",
        "          ,\"3-מרצ\"\n",
        "          ,\"4-תקווה חדשה\"\n",
        "          ,\"5-ימינה\"\n",
        "          ,\"6- ישראל ביתנו\"\n",
        "          ,\"7-כחול לבן\"\n",
        "          ,\"8-ש\\\"ס\"\n",
        "          ,\"9-הרשימה המשותפת\"\n",
        "          ,\"10-העבודה\"\n",
        "          ,\"11-רע\\\"מ\"\n",
        "          ,\"12-ח\\\"כ בודד\"],\n",
        "    \"עדה\":\n",
        "          [\"0-ספרדי\"\n",
        "          ,\"1-אשכנזי\"],\n",
        "    \"חרדי\":\n",
        "          [\"לא חרדי-0\"\n",
        "          ,\"חרדי-1\"]\n",
        "    }\n",
        "PREPROCESSORS=['N','S','C','O']#,'SC','SO','CO','SCO']\n",
        "ML_CLASSIFIERS= [ # the classifers we use\n",
        "\"RF\"\n",
        "#,\"SVM\"\n",
        ",\"MLP\"\n",
        ",\"MNB\"\n",
        ",\"LR\"\n",
        "]\n",
        "NUM_OF_WORDS=[1000,2000,3000,4000,5000]\n",
        "score=\"f1\"\n",
        "OVERSAMPLERS=[#\"none\",\n",
        "              \"random\",\n",
        "              #\"smote\",\n",
        "              #\"adasyn\"\n",
        "              ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9mEA1CnLjxO",
        "outputId": "fab21cd9-e78c-4010-8f4a-8179fac2d845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random\n",
            "מפלגה\n",
            "O\n",
            "+----+-------------------+----------+----------+----------+----------+\n",
            "|    |   Number of words |       RF |      MLP |      MNB |       LR |\n",
            "|----+-------------------+----------+----------+----------+----------|\n",
            "|  0 |              1000 | 0.108213 | 0.153857 | 0.19109  | 0.185234 |\n",
            "|  1 |              2000 | 0.117416 | 0.162423 | 0.201516 | 0.192066 |\n",
            "|  2 |              3000 | 0.119185 | 0.152075 | 0.204329 | 0.194944 |\n",
            "|  3 |              4000 | 0.114563 | 0.155445 | 0.205602 | 0.197283 |\n",
            "|  4 |              5000 | 0.116366 | 0.156604 | 0.20811  | 0.202834 |\n",
            "+----+-------------------+----------+----------+----------+----------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/pandas/core/generic.py:2703: PerformanceWarning: \n",
            "your performance may suffer as PyTables will pickle object types that it cannot\n",
            "map directly to c-types [inferred_type->integer,key->block0_values] [items->Index(['Number of words'], dtype='object')]\n",
            "\n",
            "  pytables.to_hdf(\n"
          ]
        }
      ],
      "source": [
        "for oversampling in OVERSAMPLERS:\n",
        "  print(oversampling)\n",
        "  for cls,attributes in ONE_HOT_ENCODED.items():\n",
        "    print(cls)\n",
        "    for preprocessing in PREPROCESSORS:\n",
        "      print(preprocessing)\n",
        "      df_columns = []\n",
        "      # Create a data frame with the title row\n",
        "      df_columns.append(\"Number of words\")\n",
        "      df_columns = df_columns + ML_CLASSIFIERS\n",
        "      df = pd.DataFrame(columns=df_columns)\n",
        "      # Create an array of empty strings\n",
        "      empty_strings = np.empty(len(df_columns), dtype='U64')\n",
        "      empty_strings[:] = ''\n",
        "      # Set the first element of the array to a specific value\n",
        "      empty_strings[0] = \"f1 results of \" + cls + \" by \" + preprocessing\n",
        "      title = pd.DataFrame(data=[empty_strings], columns=df.columns)\n",
        "      for num in NUM_OF_WORDS:\n",
        "        row={}\n",
        "        row[\"Number of words\"] = num\n",
        "        for classifier in ML_CLASSIFIERS:\n",
        "          row[classifier] = 0;\n",
        "          for attr in attributes:\n",
        "            ml_results = pd.read_hdf(f'/content/drive/MyDrive/model_files/results/{oversampling}/h5/{attr}_by_{preprocessing}_{score}_results.h5', key='results')\n",
        "            index = ml_results.loc[ml_results[\"Number of words\"] == num].index[0]\n",
        "            f1_score=ml_results.at[index,classifier]\n",
        "            row[classifier] = row[classifier] +f1_score\n",
        "          row[classifier] = row[classifier] / len(attributes)\n",
        "        df.loc[len(df.index)] = row\n",
        "      print(tabulate(df, headers='keys', tablefmt='psql'))\n",
        "      # Concatenate the two data frames\n",
        "      header = pd.DataFrame(data=[df_columns], columns=df_columns)\n",
        "      csv_df = pd.concat([title,header,df], ignore_index=True)\n",
        "      csv_df.to_csv(f'/content/drive/MyDrive/model_files/results/{oversampling}/csv/{cls}_by_{preprocessing}_f1_results.csv', header=False, encoding='utf-8-sig', index=False)\n",
        "      df.to_hdf(f'/content/drive/MyDrive/model_files/results/{oversampling}/h5/{cls}_by_{preprocessing}_f1_results.h5', key='results', mode='w') \n",
        "            \n",
        "            \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JngI4esLOM9y"
      },
      "source": [
        "##Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kc0YWgyOSfl"
      },
      "outputs": [],
      "source": [
        "CLASSIFY = [\n",
        "\"קבוצת גיל\"\n",
        ",\"מין\"\n",
        ",\"דעה\"\n",
        ",\"מפלגה\"\n",
        ",\"עדה\"\n",
        ",\"דתי אורתודוקסי\"\n",
        ",\"חרדי\"\n",
        ",\"יהודי\"\n",
        "]  # the classification attribute of the tweet vectors\n",
        "PREPROCESSORS=['N','S','C','O']#,'SC','SO','CO','SCO']\n",
        "ML_CLASSIFIERS= [ # the classifers we use\n",
        "\"RF\"\n",
        "#,\"SVM\"\n",
        ",\"MLP\"\n",
        ",\"MNB\"\n",
        ",\"LR\"\n",
        "]\n",
        "NUM_OF_WORDS=[1000,2000,3000,4000,5000]\n",
        "score=\"f1\"\n",
        "OVERSAMPLERS=[\"none\",\n",
        "              \"random\",\n",
        "              \"smote\",\n",
        "              \"adasyn\"\n",
        "              ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Best of each chart"
      ],
      "metadata": {
        "id": "OwzReeM1_MYq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Inm51AQXO6IS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3cf866e-b590-414a-cb85-5ef354f67398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting oversampling none\n",
            "קבוצת גיל:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| N               | MNB          |              1000 | 0.738924 |\n",
            "| C               | MNB          |              1000 | 0.736194 |\n",
            "| S               | MNB          |              1000 | 0.732587 |\n",
            "| O               | MNB          |              1000 | 0.732443 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "מין:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| N               | MLP          |              1000 | 0.417332 |\n",
            "| O               | MLP          |              1000 | 0.405753 |\n",
            "| C               | MLP          |              1000 | 0.404232 |\n",
            "| S               | MLP          |              2000 | 0.403147 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "דעה:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| C               | MLP          |              4000 | 0.381153 |\n",
            "| O               | MLP          |              4000 | 0.379275 |\n",
            "| S               | MLP          |              4000 | 0.375237 |\n",
            "| N               | MLP          |              5000 | 0.374636 |\n",
            "+-----------------+--------------+-------------------+----------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/pandas/core/generic.py:2703: PerformanceWarning: \n",
            "your performance may suffer as PyTables will pickle object types that it cannot\n",
            "map directly to c-types [inferred_type->mixed-integer,key->block1_values] [items->Index(['Preprocessing', 'Classifier', 'Number of words'], dtype='object')]\n",
            "\n",
            "  pytables.to_hdf(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "מפלגה:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| C               | MLP          |              3000 | 0.159004 |\n",
            "| O               | MLP          |              5000 | 0.149158 |\n",
            "| N               | MLP          |              5000 | 0.14132  |\n",
            "| S               | MLP          |              4000 | 0.141224 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "עדה:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| O               | MLP          |              5000 | 0.482343 |\n",
            "| C               | MLP          |              2000 | 0.481961 |\n",
            "| N               | MLP          |              1000 | 0.480899 |\n",
            "| S               | MLP          |              4000 | 0.475917 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "דתי אורתודוקסי:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| C               | MLP          |              1000 | 0.202783 |\n",
            "| O               | MLP          |              1000 | 0.201015 |\n",
            "| S               | MLP          |              2000 | 0.200858 |\n",
            "| N               | MLP          |              2000 | 0.191855 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "חרדי:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| C               | LR           |              1000 | 0.455497 |\n",
            "| N               | LR           |              1000 | 0.455451 |\n",
            "| S               | LR           |              1000 | 0.455072 |\n",
            "| O               | LR           |              1000 | 0.454866 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "יהודי:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| O               | MLP          |              4000 | 0.911892 |\n",
            "| N               | MLP          |              4000 | 0.911105 |\n",
            "| C               | LR           |              1000 | 0.910557 |\n",
            "| S               | MLP          |              5000 | 0.910203 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "starting oversampling random\n",
            "קבוצת גיל:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| N               | RF           |              5000 | 0.671911 |\n",
            "| O               | RF           |              5000 | 0.66945  |\n",
            "| C               | RF           |              1000 | 0.668572 |\n",
            "| S               | RF           |              3000 | 0.665597 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "מין:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| S               | LR           |              1000 | 0.438705 |\n",
            "| N               | MNB          |              2000 | 0.433475 |\n",
            "| C               | MNB          |              1000 | 0.429693 |\n",
            "| O               | MNB          |              2000 | 0.429367 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "דעה:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| C               | MNB          |              5000 | 0.47505  |\n",
            "| O               | MNB          |              5000 | 0.452329 |\n",
            "| N               | MNB          |              4000 | 0.446366 |\n",
            "| S               | MNB          |              5000 | 0.446242 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "מפלגה:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| C               | MNB          |              4000 | 0.219324 |\n",
            "| O               | MNB          |              5000 | 0.20811  |\n",
            "| S               | MNB          |              5000 | 0.200305 |\n",
            "| N               | MNB          |              5000 | 0.200199 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "עדה:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| O               | MNB          |              3000 | 0.490524 |\n",
            "| N               | MNB          |              3000 | 0.490395 |\n",
            "| C               | LR           |              1000 | 0.490085 |\n",
            "| S               | LR           |              2000 | 0.488405 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "דתי אורתודוקסי:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| C               | MNB          |              5000 | 0.245545 |\n",
            "| O               | MNB          |              3000 | 0.234279 |\n",
            "| S               | MNB          |              5000 | 0.228387 |\n",
            "| N               | MNB          |              3000 | 0.220297 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "חרדי:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| N               | RF           |              1000 | 0.455611 |\n",
            "| S               | RF           |              4000 | 0.455463 |\n",
            "| O               | RF           |              2000 | 0.455458 |\n",
            "| C               | RF           |              4000 | 0.455078 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "יהודי:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| S               | RF           |              1000 | 0.913177 |\n",
            "| C               | RF           |              1000 | 0.912232 |\n",
            "| N               | RF           |              1000 | 0.911761 |\n",
            "| O               | RF           |              2000 | 0.911181 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "starting oversampling smote\n",
            "קבוצת גיל:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| O               | RF           |              5000 | 0.694114 |\n",
            "| N               | RF           |              5000 | 0.693953 |\n",
            "| S               | RF           |              5000 | 0.69156  |\n",
            "| C               | RF           |              2000 | 0.687205 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "מין:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| N               | LR           |              1000 | 0.447939 |\n",
            "| S               | MNB          |              1000 | 0.435446 |\n",
            "| O               | MNB          |              2000 | 0.435271 |\n",
            "| C               | MNB          |              2000 | 0.433672 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "דעה:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| C               | MNB          |              5000 | 0.466681 |\n",
            "| O               | MNB          |              4000 | 0.449441 |\n",
            "| S               | MNB          |              5000 | 0.446777 |\n",
            "| N               | MNB          |              5000 | 0.44503  |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "מפלגה:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| C               | MNB          |              5000 | 0.220464 |\n",
            "| O               | MNB          |              5000 | 0.207053 |\n",
            "| N               | MNB          |              5000 | 0.203377 |\n",
            "| S               | MNB          |              5000 | 0.202205 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "עדה:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| S               | LR           |              1000 | 0.490822 |\n",
            "| C               | LR           |              1000 | 0.488668 |\n",
            "| O               | MNB          |              3000 | 0.488426 |\n",
            "| N               | LR           |              3000 | 0.488394 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "דתי אורתודוקסי:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| C               | MNB          |              3000 | 0.243015 |\n",
            "| O               | MNB          |              3000 | 0.242956 |\n",
            "| N               | MNB          |              3000 | 0.237437 |\n",
            "| S               | MNB          |              4000 | 0.232323 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "חרדי:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| C               | RF           |              4000 | 0.454591 |\n",
            "| S               | RF           |              5000 | 0.454218 |\n",
            "| O               | RF           |              4000 | 0.45412  |\n",
            "| N               | RF           |              5000 | 0.454085 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "יהודי:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| S               | RF           |              3000 | 0.912917 |\n",
            "| O               | LR           |              5000 | 0.911455 |\n",
            "| C               | LR           |              5000 | 0.911247 |\n",
            "| N               | RF           |              2000 | 0.911086 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "starting oversampling adasyn\n",
            "קבוצת גיל:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| C               | MNB          |              1000 | 0.733837 |\n",
            "| N               | MNB          |              1000 | 0.717666 |\n",
            "| O               | MNB          |              1000 | 0.710665 |\n",
            "| S               | MNB          |              3000 | 0.703825 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "מין:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| N               | MNB          |              2000 | 0.446815 |\n",
            "| C               | MNB          |              1000 | 0.441056 |\n",
            "| O               | LR           |              2000 | 0.440632 |\n",
            "| S               | MNB          |              1000 | 0.435923 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "דעה:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| C               | MNB          |              5000 | 0.469297 |\n",
            "| O               | MNB          |              5000 | 0.448624 |\n",
            "| S               | MNB          |              4000 | 0.444282 |\n",
            "| N               | MNB          |              4000 | 0.443809 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "מפלגה:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| C               | MNB          |              4000 | 0.219125 |\n",
            "| O               | MNB          |              5000 | 0.210642 |\n",
            "| N               | MNB          |              5000 | 0.203882 |\n",
            "| S               | MNB          |              5000 | 0.203185 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "עדה:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| N               | LR           |              4000 | 0.500837 |\n",
            "| C               | LR           |              1000 | 0.493704 |\n",
            "| S               | LR           |              1000 | 0.491474 |\n",
            "| O               | MNB          |              3000 | 0.488851 |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "דתי אורתודוקסי:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| C               | LR           |              2000 | 0.24858  |\n",
            "| O               | MNB          |              3000 | 0.246719 |\n",
            "| N               | MNB          |              5000 | 0.23819  |\n",
            "| S               | MNB          |              5000 | 0.23209  |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "חרדי:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| C               | RF           |              5000 | 0.454752 |\n",
            "| S               | RF           |              5000 | 0.454487 |\n",
            "| O               | RF           |              5000 | 0.4542   |\n",
            "| N               | RF           |              4000 | 0.45367  |\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "יהודי:\n",
            "+-----------------+--------------+-------------------+----------+\n",
            "| Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|-----------------+--------------+-------------------+----------|\n",
            "| S               | RF           |              3000 | 0.91317  |\n",
            "| C               | LR           |              5000 | 0.912056 |\n",
            "| O               | RF           |              3000 | 0.910786 |\n",
            "| N               | RF           |              3000 | 0.910626 |\n",
            "+-----------------+--------------+-------------------+----------+\n"
          ]
        }
      ],
      "source": [
        "df_columns = [\"Preprocessing\",\"Classifier\",\"Number of words\",\"Score\"]\n",
        "for oversampling in OVERSAMPLERS:\n",
        "  print(f\"starting oversampling {oversampling}\")\n",
        "  for cls in CLASSIFY:\n",
        "    print(f\"{cls}:\")\n",
        "    summary = pd.DataFrame(columns=df_columns)\n",
        "    for preprocessing in PREPROCESSORS:\n",
        "        ml_results = pd.read_hdf(f'/content/drive/MyDrive/model_files/results/{oversampling}/h5/{cls}_by_{preprocessing}_{score}_results.h5', key='results')\n",
        "        max_value_index = ml_results[ml_results.columns[1:]].stack().idxmax()\n",
        "        row_index= int(max_value_index[0])\n",
        "        column_index= ml_results.columns.get_loc(max_value_index[1])\n",
        "        row={\"Preprocessing\": preprocessing,\"Classifier\": max_value_index[1],\"Number of words\": ml_results.iat[row_index,0],\"Score\":ml_results.iat[row_index,column_index] }\n",
        "        summary.loc[len(summary.index)] = row\n",
        "    summary = summary.sort_values('Score', ascending=False)\n",
        "    print(tabulate(summary, headers='keys', tablefmt='psql',showindex=False))\n",
        "    summary.to_csv(f'/content/drive/MyDrive/model_files/summary/charts/{oversampling}/csv/{cls}_{score}_results_summary.csv', encoding='utf-8-sig', index=False)\n",
        "    summary.to_hdf(f'/content/drive/MyDrive/model_files/summary/charts/{oversampling}/h5/{cls}_{score}_results_summary.h5', key='summary', mode='w') \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Best of each class"
      ],
      "metadata": {
        "id": "rtfR8uWX_Yyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_columns = [\"Oversampling\",\"Preprocessing\",\"Classifier\",\"Number of words\",\"Score\"]\n",
        "for cls in CLASSIFY:\n",
        "    print(f\"{cls}:\")\n",
        "    class_summary = pd.DataFrame(columns=df_columns)\n",
        "    for oversampling in OVERSAMPLERS:\n",
        "      summary= pd.read_hdf(f'/content/drive/MyDrive/model_files/summary/charts/{oversampling}/h5/{cls}_{score}_results_summary.h5', key='summary')\n",
        "      summary[\"Oversampling\"] = oversampling\n",
        "      class_summary = pd.concat([class_summary,summary], ignore_index=True)\n",
        "    class_summary = class_summary.sort_values('Score', ascending=False)\n",
        "    class_summary=class_summary.head(4)\n",
        "    print(tabulate(class_summary, headers='keys', tablefmt='psql',showindex=False))\n",
        "    class_summary.to_csv(f'/content/drive/MyDrive/model_files/summary/classes/csv/{cls}_{score}_results_summary.csv', encoding='utf-8-sig', index=False)\n",
        "    class_summary.to_hdf(f'/content/drive/MyDrive/model_files/summary/classes/h5/{cls}_{score}_results_summary.h5', key='class_summary', mode='w')      \n",
        "\n",
        "\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl5w6Q99B-rx",
        "outputId": "66369ae8-8356-4417-aeb2-b65860c0a19d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "קבוצת גיל:\n",
            "+----------------+-----------------+--------------+-------------------+----------+\n",
            "| Oversampling   | Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|----------------+-----------------+--------------+-------------------+----------|\n",
            "| none           | N               | MNB          |              1000 | 0.738924 |\n",
            "| none           | C               | MNB          |              1000 | 0.736194 |\n",
            "| adasyn         | C               | MNB          |              1000 | 0.733837 |\n",
            "| none           | S               | MNB          |              1000 | 0.732587 |\n",
            "+----------------+-----------------+--------------+-------------------+----------+\n",
            "מין:\n",
            "+----------------+-----------------+--------------+-------------------+----------+\n",
            "| Oversampling   | Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|----------------+-----------------+--------------+-------------------+----------|\n",
            "| smote          | N               | LR           |              1000 | 0.447939 |\n",
            "| adasyn         | N               | MNB          |              2000 | 0.446815 |\n",
            "| adasyn         | C               | MNB          |              1000 | 0.441056 |\n",
            "| adasyn         | O               | LR           |              2000 | 0.440632 |\n",
            "+----------------+-----------------+--------------+-------------------+----------+\n",
            "דעה:\n",
            "+----------------+-----------------+--------------+-------------------+----------+\n",
            "| Oversampling   | Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|----------------+-----------------+--------------+-------------------+----------|\n",
            "| random         | C               | MNB          |              5000 | 0.47505  |\n",
            "| adasyn         | C               | MNB          |              5000 | 0.469297 |\n",
            "| smote          | C               | MNB          |              5000 | 0.466681 |\n",
            "| random         | O               | MNB          |              5000 | 0.452329 |\n",
            "+----------------+-----------------+--------------+-------------------+----------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/pandas/core/generic.py:2703: PerformanceWarning: \n",
            "your performance may suffer as PyTables will pickle object types that it cannot\n",
            "map directly to c-types [inferred_type->mixed-integer,key->block1_values] [items->Index(['Oversampling', 'Preprocessing', 'Classifier', 'Number of words'], dtype='object')]\n",
            "\n",
            "  pytables.to_hdf(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "מפלגה:\n",
            "+----------------+-----------------+--------------+-------------------+----------+\n",
            "| Oversampling   | Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|----------------+-----------------+--------------+-------------------+----------|\n",
            "| smote          | C               | MNB          |              5000 | 0.220464 |\n",
            "| random         | C               | MNB          |              4000 | 0.219324 |\n",
            "| adasyn         | C               | MNB          |              4000 | 0.219125 |\n",
            "| adasyn         | O               | MNB          |              5000 | 0.210642 |\n",
            "+----------------+-----------------+--------------+-------------------+----------+\n",
            "עדה:\n",
            "+----------------+-----------------+--------------+-------------------+----------+\n",
            "| Oversampling   | Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|----------------+-----------------+--------------+-------------------+----------|\n",
            "| adasyn         | N               | LR           |              4000 | 0.500837 |\n",
            "| adasyn         | C               | LR           |              1000 | 0.493704 |\n",
            "| adasyn         | S               | LR           |              1000 | 0.491474 |\n",
            "| smote          | S               | LR           |              1000 | 0.490822 |\n",
            "+----------------+-----------------+--------------+-------------------+----------+\n",
            "דתי אורתודוקסי:\n",
            "+----------------+-----------------+--------------+-------------------+----------+\n",
            "| Oversampling   | Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|----------------+-----------------+--------------+-------------------+----------|\n",
            "| adasyn         | C               | LR           |              2000 | 0.24858  |\n",
            "| adasyn         | O               | MNB          |              3000 | 0.246719 |\n",
            "| random         | C               | MNB          |              5000 | 0.245545 |\n",
            "| smote          | C               | MNB          |              3000 | 0.243015 |\n",
            "+----------------+-----------------+--------------+-------------------+----------+\n",
            "חרדי:\n",
            "+----------------+-----------------+--------------+-------------------+----------+\n",
            "| Oversampling   | Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|----------------+-----------------+--------------+-------------------+----------|\n",
            "| random         | N               | RF           |              1000 | 0.455611 |\n",
            "| none           | C               | LR           |              1000 | 0.455497 |\n",
            "| random         | S               | RF           |              4000 | 0.455463 |\n",
            "| random         | O               | RF           |              2000 | 0.455458 |\n",
            "+----------------+-----------------+--------------+-------------------+----------+\n",
            "יהודי:\n",
            "+----------------+-----------------+--------------+-------------------+----------+\n",
            "| Oversampling   | Preprocessing   | Classifier   |   Number of words |    Score |\n",
            "|----------------+-----------------+--------------+-------------------+----------|\n",
            "| random         | S               | RF           |              1000 | 0.913177 |\n",
            "| adasyn         | S               | RF           |              3000 | 0.91317  |\n",
            "| smote          | S               | RF           |              3000 | 0.912917 |\n",
            "| random         | C               | RF           |              1000 | 0.912232 |\n",
            "+----------------+-----------------+--------------+-------------------+----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Best Configurations"
      ],
      "metadata": {
        "id": "p_lAzLFi9q4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oversamplers_dict={}\n",
        "preprocesses_dict={}\n",
        "classifiers_dict={}\n",
        "num_of_words_dict={}\n",
        "for cls in CLASSIFY:\n",
        "    class_summary= pd.read_hdf(f'/content/drive/MyDrive/model_files/summary/classes/h5/{cls}_{score}_results_summary.h5', key='class_summary')\n",
        "    class_summary = class_summary.reset_index()\n",
        "    for i in range(len(class_summary.index)):\n",
        "      factor = len(class_summary.index) - i\n",
        "      oversmpling = class_summary.at[i,\"Oversampling\"]\n",
        "      preprocess = class_summary.at[i,\"Preprocessing\"]\n",
        "      classifier = class_summary.at[i,\"Classifier\"]\n",
        "      num_of_words = class_summary.at[i,\"Number of words\"]\n",
        "\n",
        "      if oversmpling in oversamplers_dict:\n",
        "        oversamplers_dict[oversmpling] = oversamplers_dict[oversmpling] + factor\n",
        "      else:\n",
        "        oversamplers_dict[oversmpling] = factor\n",
        "      \n",
        "      if preprocess in preprocesses_dict:\n",
        "        preprocesses_dict[preprocess] = preprocesses_dict[preprocess] + factor\n",
        "      else:\n",
        "        preprocesses_dict[preprocess] = factor\n",
        "      \n",
        "      if classifier in classifiers_dict:\n",
        "        classifiers_dict[classifier] = classifiers_dict[classifier] + factor\n",
        "      else:\n",
        "        classifiers_dict[classifier] = factor\n",
        "      \n",
        "      if num_of_words in num_of_words_dict:\n",
        "        num_of_words_dict[num_of_words] = num_of_words_dict[num_of_words] + factor\n",
        "      else:\n",
        "        num_of_words_dict[num_of_words] = factor\n",
        "\n",
        "oversamplers_dict = dict(sorted(oversamplers_dict.items(), key=lambda x: x[1],reverse=True))\n",
        "preprocesses_dict = dict(sorted(preprocesses_dict.items(), key=lambda x: x[1],reverse=True))\n",
        "classifiers_dict = dict(sorted(classifiers_dict.items(), key=lambda x: x[1],reverse=True))\n",
        "num_of_words_dict = dict(sorted(num_of_words_dict.items(), key=lambda x: x[1],reverse=True))\n",
        "\n",
        "\n",
        "print(oversamplers_dict)\n",
        "print(preprocesses_dict)\n",
        "print(classifiers_dict)\n",
        "print(num_of_words_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KDGQRxp-FL3",
        "outputId": "650d8d3b-2dcb-4d6a-8553-21b61b365b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'adasyn': 33, 'random': 22, 'smote': 14, 'none': 11}\n",
            "{'C': 39, 'N': 19, 'S': 15, 'O': 7}\n",
            "{'MNB': 41, 'LR': 22, 'RF': 17}\n",
            "{1000.0: 34, 5000: 17, 4000: 11, 2000.0: 9, 3000.0: 9}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}